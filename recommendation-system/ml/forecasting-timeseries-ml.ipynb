{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6d4eda",
   "metadata": {},
   "source": [
    "# AutoML: Train \"the best\" Time-Series Forecasting model for Retail Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f23d42",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c55927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress OpenTelemetry warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Overriding of current\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Attempting to instrument\")\n",
    "\n",
    "# Suppress Azure SDK telemetry logging\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac07cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "credential = AzureCliCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    subscription_id = \"57123c17-af1a-4ec2-9494-a214fb148bf4\"\n",
    "    resource_group = \"admin-rg\"\n",
    "    workspace = \"ml-demo-wksp-wus-01\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "except Exception as ex:\n",
    "    print(\"Ex:\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection\n",
    "ws = ml_client.workspaces.get(ml_client.workspace_name)\n",
    "print(f\"Connected to: {ws.name} ({ws.location})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ebeb9",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "Using [Retail data analytics](https://www.kaggle.com/datasets/manjeetsingh/retaildataset) - weekly sales by store and department."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e5c30",
   "metadata": {},
   "source": [
    "## 2.1 Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a98900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "stores_df = pd.read_csv('../dataset/stores data-set.csv')\n",
    "features_df = pd.read_csv('../dataset/Features data set.csv')\n",
    "sales_df = pd.read_csv('../dataset/sales data-set.csv')\n",
    "\n",
    "# Quick exploration\n",
    "print(f\"Stores: {stores_df.shape}\")\n",
    "print(f\"Features: {features_df.shape}\")\n",
    "print(f\"Sales: {sales_df.shape}\")\n",
    "\n",
    "print(\"\\n--- Stores Data ---\")\n",
    "display(stores_df.head())\n",
    "\n",
    "print(\"\\n--- Features Data ---\")\n",
    "display(features_df.head())\n",
    "\n",
    "print(\"\\n--- Sales Data ---\")\n",
    "display(sales_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffc88b",
   "metadata": {},
   "source": [
    "## 2.2 Merge Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales with stores (on Store)\n",
    "merged_df = sales_df.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "# Merge with features (on Store and Date)\n",
    "merged_df = merged_df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
    "\n",
    "# Drop duplicate IsHoliday column from features\n",
    "merged_df = merged_df.drop(columns=['IsHoliday_feat'])\n",
    "\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "print(f\"\\nColumns: {merged_df.columns.tolist()}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e3f01",
   "metadata": {},
   "source": [
    "## 2.3 Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to datetime (format is dd/mm/yyyy)\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "\n",
    "# Extract date features\n",
    "merged_df['Year'] = merged_df['Date'].dt.year\n",
    "merged_df['Month'] = merged_df['Date'].dt.month\n",
    "merged_df['Week'] = merged_df['Date'].dt.isocalendar().week\n",
    "merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "# Handle missing MarkDown values (only available after Nov 2011)\n",
    "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "merged_df[markdown_cols] = merged_df[markdown_cols].fillna(0)\n",
    "\n",
    "# Encode categorical: Store Type (A, B, C)\n",
    "if 'Type' in merged_df.columns:\n",
    "    merged_df = pd.get_dummies(merged_df, columns=['Type'], prefix='StoreType')\n",
    "\n",
    "print(f\"Feature engineered dataset: {merged_df.shape}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0205d",
   "metadata": {},
   "source": [
    "## 2.4 Train/Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ab93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date\n",
    "merged_df = merged_df.sort_values(['Store', 'Dept', 'Date'])\n",
    "\n",
    "# Time-based split: train on data before 2012, validate on 2012\n",
    "train_df = merged_df[merged_df['Year'] < 2012].copy()\n",
    "validation_df = merged_df[merged_df['Year'] >= 2012].copy()\n",
    "\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Validation set: {validation_df.shape}\")\n",
    "print(f\"\\nTrain date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
    "print(f\"Validation date range: {validation_df['Date'].min()} to {validation_df['Date'].max()}\")\n",
    "print(f\"\\nTrain/Validation split ratio: {len(train_df)/(len(train_df)+len(validation_df))*100:.1f}% / {len(validation_df)/(len(train_df)+len(validation_df))*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00bca9",
   "metadata": {},
   "source": [
    "## 2.5 Prepare MLTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Rename columns for AutoML compatibility\n",
    "train_df = train_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "validation_df = validation_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('./data/training-mltable-folder', exist_ok=True)\n",
    "os.makedirs('./data/validation-mltable-folder', exist_ok=True)\n",
    "\n",
    "# Save as CSV (MLTable will reference these)\n",
    "train_df.to_csv('./data/training-mltable-folder/train.csv', index=False)\n",
    "validation_df.to_csv('./data/validation-mltable-folder/validation.csv', index=False)\n",
    "\n",
    "print(f\"Training data saved to: ./data/training-mltable-folder/train.csv\")\n",
    "print(f\"Validation data saved to: ./data/validation-mltable-folder/validation.csv\")\n",
    "print(f\"\\nColumns in final datasets:\")\n",
    "print(train_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLTable YAML files for Azure ML\n",
    "\n",
    "mltable_train = \"\"\"paths:\n",
    "  - file: ./train.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "mltable_val = \"\"\"paths:\n",
    "  - file: ./validation.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "with open('./data/training-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_train)\n",
    "    \n",
    "with open('./data/validation-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_val)\n",
    "\n",
    "print(\"MLTable files created:\")\n",
    "print(\"  - ./data/training-mltable-folder/MLTable\")\n",
    "print(\"  - ./data/validation-mltable-folder/MLTable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861505be",
   "metadata": {},
   "source": [
    "## 2.6 Upload to Azure Blob Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80df692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Azure Storage configuration\n",
    "STORAGE_ACCOUNT = \"mldemowkspwus02609576373\"\n",
    "CONTAINER = \"azureml-blobstore-cff56e3a-d016-4526-aa58-71c460675066\"\n",
    "\n",
    "def upload_to_blob(source_folder, destination_path):\n",
    "    \"\"\"Upload local folder to Azure Blob Storage using OAuth authentication.\"\"\"\n",
    "    cmd = [\n",
    "        \"az\", \"storage\", \"blob\", \"upload-batch\",\n",
    "        \"--account-name\", STORAGE_ACCOUNT,\n",
    "        \"--destination\", f\"{CONTAINER}/{destination_path}\",\n",
    "        \"--source\", source_folder,\n",
    "        \"--auth-mode\", \"login\",\n",
    "        \"--overwrite\"\n",
    "    ]\n",
    "    print(f\"Uploading {source_folder} to {destination_path}...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ Successfully uploaded to {destination_path}\")\n",
    "    else:\n",
    "        print(f\"✗ Upload failed: {result.stderr}\")\n",
    "    return result.returncode == 0\n",
    "\n",
    "# Upload training data\n",
    "upload_to_blob(\"./data/training-mltable-folder\", \"retail-training-data\")\n",
    "\n",
    "# Upload validation data\n",
    "upload_to_blob(\"./data/validation-mltable-folder\", \"retail-validation-data\")\n",
    "\n",
    "print(\"\\nData upload complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc993c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference data already uploaded to blob storage (via Azure CLI with OAuth)\n",
    "# Using identity-based datastore (no SAS tokens required)\n",
    "\n",
    "my_training_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, \n",
    "    path=\"azureml://datastores/workspaceblobstore_identity/paths/retail-training-data\"\n",
    ")\n",
    "\n",
    "my_validation_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, \n",
    "    path=\"azureml://datastores/workspaceblobstore_identity/paths/retail-validation-data\"\n",
    ")\n",
    "\n",
    "print(\"Using pre-uploaded data from identity-based datastore:\")\n",
    "print(\"  - Training: azureml://datastores/workspaceblobstore_identity/paths/retail-training-data\")\n",
    "print(\"  - Validation: azureml://datastores/workspaceblobstore_identity/paths/retail-validation-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929c4a9",
   "metadata": {},
   "source": [
    "# 3. Configure and Run AutoML Forecasting Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8836281",
   "metadata": {},
   "source": [
    "## 3.1 Job Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a03eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general job parameters\n",
    "max_trials = 5\n",
    "exp_name = \"sales-forecasting-experiment\"\n",
    "\n",
    "# Compute cluster name (must exist in your Azure ML workspace)\n",
    "compute_name = \"teslat4-gpu-wus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AutoML forecasting job with the related factory-function.\n",
    "forecasting_job = automl.forecasting(\n",
    "    experiment_name=exp_name,\n",
    "    compute=compute_name,  \n",
    "    training_data=my_training_data_input,\n",
    "    validation_data=my_validation_data_input, \n",
    "    target_column_name=\"demand\",\n",
    "    primary_metric=\"NormalizedRootMeanSquaredError\",\n",
    "    enable_model_explainability=True,\n",
    "    tags={\"retail\": \"forecasting\"},\n",
    ")\n",
    "\n",
    "# Limits are all optional\n",
    "forecasting_job.set_limits(\n",
    "    timeout_minutes=600,\n",
    "    trial_timeout_minutes=20,\n",
    "    max_trials=max_trials,\n",
    "    enable_early_termination=True,\n",
    ")\n",
    "\n",
    "# Specialized properties for Time Series Forecasting training\n",
    "forecasting_job.set_forecast_settings(\n",
    "    time_column_name=\"timeStamp\",\n",
    "    forecast_horizon=12,  # 12 weeks forecast (reduced for data consistency)\n",
    "    frequency=\"W-FRI\",    # Weekly frequency anchored to Friday (retail week ending)\n",
    "    target_lags=[1, 2, 4],  # Lag features for 1, 2, and 4 weeks back\n",
    "    target_rolling_window_size=4,\n",
    "    time_series_id_column_names=[\"Store\", \"Dept\"],\n",
    "    short_series_handling_config=\"auto\",  # Handle series with irregular/missing data\n",
    ")\n",
    "\n",
    "forecasting_job.set_training(blocked_training_algorithms=[\"ExtremeRandomTrees\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16457be8",
   "metadata": {},
   "source": [
    "## 3.2 Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40118475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the AutoML job\n",
    "returned_job = ml_client.jobs.create_or_update(forecasting_job)\n",
    "print(f\"Created job: {returned_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8129921",
   "metadata": {},
   "source": [
    "# 4. Get Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b636706",
   "metadata": {},
   "source": [
    "## 4.1 Download Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b90dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to complete (if not already)\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "# Get the completed job details\n",
    "completed_job = ml_client.jobs.get(returned_job.name)\n",
    "print(f\"Job status: {completed_job.status}\")\n",
    "\n",
    "# Download the best model artifacts\n",
    "model_download_path = \"./outputs/best_model\"\n",
    "os.makedirs(model_download_path, exist_ok=True)\n",
    "\n",
    "ml_client.jobs.download(\n",
    "    name=returned_job.name,\n",
    "    download_path=\"./outputs\",\n",
    "    output_name=\"best_model\"\n",
    ")\n",
    "\n",
    "print(f\"Best model downloaded to: {model_download_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08e102",
   "metadata": {},
   "source": [
    "## 4.2 Load Model and Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d1291",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b922eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Load the downloaded model\n",
    "model_path = \"./outputs/best_model\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "print(f\"Model loaded successfully from: {model_path}\")\n",
    "print(f\"Model flavor: {loaded_model.metadata.flavors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data for prediction (remove target column)\n",
    "prediction_input = validation_df.drop(columns=['demand']).copy()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = loaded_model.predict(prediction_input)\n",
    "\n",
    "# Add predictions to validation dataframe\n",
    "validation_df['predicted_demand'] = predictions\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions\")\n",
    "display(validation_df[['Store', 'Dept', 'timeStamp', 'demand', 'predicted_demand']].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612aef4",
   "metadata": {},
   "source": [
    "## 4.3 Evaluate Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "actual_tensor = torch.tensor(validation_df['demand'].values, dtype=torch.float32)\n",
    "predicted_tensor = torch.tensor(validation_df['predicted_demand'].values, dtype=torch.float32)\n",
    "\n",
    "# Calculate evaluation metrics using PyTorch\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = torch.mean(torch.abs(actual_tensor - predicted_tensor)).item()\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "mse = torch.mean((actual_tensor - predicted_tensor) ** 2)\n",
    "rmse = torch.sqrt(mse).item()\n",
    "\n",
    "# R² Score\n",
    "ss_res = torch.sum((actual_tensor - predicted_tensor) ** 2)\n",
    "ss_tot = torch.sum((actual_tensor - torch.mean(actual_tensor)) ** 2)\n",
    "r2 = (1 - ss_res / ss_tot).item()\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE) - handle zeros\n",
    "non_zero_mask = actual_tensor != 0\n",
    "mape = torch.mean(torch.abs((actual_tensor[non_zero_mask] - predicted_tensor[non_zero_mask]) / actual_tensor[non_zero_mask])).item() * 100\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION METRICS (PyTorch)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Absolute Error (MAE):        ${mae:,.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE):   ${rmse:,.2f}\")\n",
    "print(f\"R² Score:                          {r2:.4f}\")\n",
    "print(f\"Mean Absolute % Error (MAPE):      {mape:.2f}%\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97837621",
   "metadata": {},
   "source": [
    "## 4.4 Save Predictions and Register Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "output_file = \"./outputs/predictions.csv\"\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "# Select relevant columns for output\n",
    "prediction_output = validation_df[['Store', 'Dept', 'timeStamp', 'demand', 'predicted_demand']].copy()\n",
    "prediction_output['error'] = prediction_output['demand'] - prediction_output['predicted_demand']\n",
    "prediction_output['absolute_error'] = abs(prediction_output['error'])\n",
    "\n",
    "prediction_output.to_csv(output_file, index=False)\n",
    "print(f\"Predictions saved to: {output_file}\")\n",
    "print(f\"Total predictions: {len(prediction_output)}\")\n",
    "\n",
    "# Show summary by store\n",
    "print(\"\\n--- Prediction Summary by Store (Top 10) ---\")\n",
    "store_summary = prediction_output.groupby('Store').agg({\n",
    "    'demand': 'sum',\n",
    "    'predicted_demand': 'sum',\n",
    "    'absolute_error': 'mean'\n",
    "}).round(2)\n",
    "store_summary.columns = ['Actual Sales', 'Predicted Sales', 'Avg Absolute Error']\n",
    "display(store_summary.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the best model in Azure ML Model Registry\n",
    "model = Model(\n",
    "    path=f\"azureml://jobs/{returned_job.name}/outputs/best_model\",\n",
    "    name=\"retail-sales-forecasting-model\",\n",
    "    description=\"AutoML time-series forecasting model for retail weekly sales\",\n",
    "    type=\"mlflow_model\"\n",
    ")\n",
    "\n",
    "registered_model = ml_client.models.create_or_update(model)\n",
    "print(f\"Registered model: {registered_model.name}, version: {registered_model.version}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
