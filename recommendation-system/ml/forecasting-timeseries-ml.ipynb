{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6d4eda",
   "metadata": {},
   "source": [
    "# AutoML: Train \"the best\" Time-Series Forecasting model for Retail Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f23d42",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c55927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress OpenTelemetry warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Overriding of current\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Attempting to instrument\")\n",
    "\n",
    "# Suppress Azure SDK telemetry logging\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "credential = AzureCliCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    subscription_id = \"57123c17-af1a-4ec2-9494-a214fb148bf4\"\n",
    "    resource_group = \"admin-rg\"\n",
    "    workspace = \"ml-demo-wksp-wus-01\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "except Exception as ex:\n",
    "    print(\"Ex:\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection\n",
    "ws = ml_client.workspaces.get(ml_client.workspace_name)\n",
    "print(f\"Connected to: {ws.name} ({ws.location})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ebeb9",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "Using [Retail data analytics](https://www.kaggle.com/datasets/manjeetsingh/retaildataset) - weekly sales by store and department."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e5c30",
   "metadata": {},
   "source": [
    "## 2.1 Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a98900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "stores_df = pd.read_csv('../dataset/stores data-set.csv')\n",
    "features_df = pd.read_csv('../dataset/Features data set.csv')\n",
    "sales_df = pd.read_csv('../dataset/sales data-set.csv')\n",
    "\n",
    "# Quick exploration\n",
    "print(f\"Stores: {stores_df.shape}\")\n",
    "print(f\"Features: {features_df.shape}\")\n",
    "print(f\"Sales: {sales_df.shape}\")\n",
    "\n",
    "print(\"\\n--- Stores Data ---\")\n",
    "display(stores_df.head())\n",
    "\n",
    "print(\"\\n--- Features Data ---\")\n",
    "display(features_df.head())\n",
    "\n",
    "print(\"\\n--- Sales Data ---\")\n",
    "display(sales_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffc88b",
   "metadata": {},
   "source": [
    "## 2.2 Merge Datasets\n",
    "Merge sales with stores (on Store) and then with features (on Store and Date).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales with stores (on Store)\n",
    "merged_df = sales_df.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "# Merge with features (on Store and Date)\n",
    "merged_df = merged_df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
    "\n",
    "# Drop duplicate IsHoliday column from features\n",
    "merged_df = merged_df.drop(columns=['IsHoliday_feat'])\n",
    "\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "print(f\"\\nColumns: {merged_df.columns.tolist()}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e3f01",
   "metadata": {},
   "source": [
    "## 2.3 Feature Engineering\n",
    "Create new features from date, handle missing MarkDown values, and encode categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to datetime (format is dd/mm/yyyy)\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "\n",
    "# Extract date features\n",
    "merged_df['Year'] = merged_df['Date'].dt.year\n",
    "merged_df['Month'] = merged_df['Date'].dt.month\n",
    "merged_df['Week'] = merged_df['Date'].dt.isocalendar().week\n",
    "merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "# Handle missing MarkDown values (only available after Nov 2011)\n",
    "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "merged_df[markdown_cols] = merged_df[markdown_cols].fillna(0)\n",
    "\n",
    "# Encode categorical: Store Type (A, B, C)\n",
    "if 'Type' in merged_df.columns:\n",
    "    merged_df = pd.get_dummies(merged_df, columns=['Type'], prefix='StoreType')\n",
    "\n",
    "print(f\"Feature engineered dataset: {merged_df.shape}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0205d",
   "metadata": {},
   "source": [
    "## 2.4 Time-Based Train/Validation Split\n",
    "\n",
    "Split data chronologically: train on data before 2012, validate on 2012 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ab93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "dupes = merged_df.groupby(['Store', 'Dept', 'Date']).size()\n",
    "print(f\"Duplicate combinations: {(dupes > 1).sum()}\")\n",
    "\n",
    "# Aggregate duplicates: sum Weekly_Sales, take first for other columns\n",
    "agg_funcs = {col: 'first' for col in merged_df.columns if col not in ['Store', 'Dept', 'Date']}\n",
    "agg_funcs['Weekly_Sales'] = 'sum'\n",
    "merged_df = merged_df.groupby(['Store', 'Dept', 'Date'], as_index=False).agg(agg_funcs)\n",
    "print(f\"After deduplication: {merged_df.shape}\")\n",
    "\n",
    "# Sort by date\n",
    "merged_df = merged_df.sort_values(['Store', 'Dept', 'Date'])\n",
    "\n",
    "# Time-based split: train on data before 2012, validate on 2012\n",
    "train_df = merged_df[merged_df['Year'] < 2012].copy()\n",
    "validation_df = merged_df[merged_df['Year'] >= 2012].copy()\n",
    "\n",
    "print(f\"\\nTraining set: {train_df.shape}\")\n",
    "print(f\"Validation set: {validation_df.shape}\")\n",
    "print(f\"Train date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
    "print(f\"Validation date range: {validation_df['Date'].min()} to {validation_df['Date'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00bca9",
   "metadata": {},
   "source": [
    "## 2.5 Prepare Data for Azure ML AutoML\n",
    "\n",
    "Rename columns to match AutoML expectations and save as MLTable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Rename columns for AutoML compatibility\n",
    "train_df = train_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "validation_df = validation_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "\n",
    "# Create single time series ID column (before converting dates to strings)\n",
    "train_df['ts_id'] = train_df['Store'].astype(str) + '_' + train_df['Dept'].astype(str)\n",
    "validation_df['ts_id'] = validation_df['Store'].astype(str) + '_' + validation_df['Dept'].astype(str)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Filter to common time series IDs (both train and validation must have same IDs)\n",
    "# ============================================================================\n",
    "train_ts_ids = set(train_df['ts_id'].unique())\n",
    "val_ts_ids = set(validation_df['ts_id'].unique())\n",
    "\n",
    "val_only_ids = val_ts_ids - train_ts_ids\n",
    "train_only_ids = train_ts_ids - val_ts_ids\n",
    "common_ids = train_ts_ids & val_ts_ids\n",
    "\n",
    "print(f\"=== Step 1: Filter to common time series ===\")\n",
    "print(f\"Time series in training only: {len(train_only_ids)}\")\n",
    "print(f\"Time series in validation only: {len(val_only_ids)}\")\n",
    "print(f\"Common time series: {len(common_ids)}\")\n",
    "\n",
    "# Keep only common IDs\n",
    "train_df = train_df[train_df['ts_id'].isin(common_ids)]\n",
    "validation_df = validation_df[validation_df['ts_id'].isin(common_ids)]\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Check contiguity - validation must start right after training ends\n",
    "# For weekly data, the gap should be exactly 7 days\n",
    "# ============================================================================\n",
    "print(f\"\\n=== Step 2: Check contiguity (no gaps between train and validation) ===\")\n",
    "\n",
    "# Get max date per ts_id in training\n",
    "train_max_dates = train_df.groupby('ts_id')['timeStamp'].max().reset_index()\n",
    "train_max_dates.columns = ['ts_id', 'train_max_date']\n",
    "\n",
    "# Get min date per ts_id in validation\n",
    "val_min_dates = validation_df.groupby('ts_id')['timeStamp'].min().reset_index()\n",
    "val_min_dates.columns = ['ts_id', 'val_min_date']\n",
    "\n",
    "# Merge to compare\n",
    "contiguity_check = train_max_dates.merge(val_min_dates, on='ts_id')\n",
    "contiguity_check['train_max_date'] = pd.to_datetime(contiguity_check['train_max_date'])\n",
    "contiguity_check['val_min_date'] = pd.to_datetime(contiguity_check['val_min_date'])\n",
    "contiguity_check['gap_days'] = (contiguity_check['val_min_date'] - contiguity_check['train_max_date']).dt.days\n",
    "\n",
    "# For weekly data, gap should be 7 days (next week)\n",
    "# Allow some flexibility: 6-8 days is acceptable\n",
    "contiguity_check['is_contiguous'] = contiguity_check['gap_days'].between(6, 8)\n",
    "\n",
    "non_contiguous = contiguity_check[~contiguity_check['is_contiguous']]\n",
    "contiguous_ids = set(contiguity_check[contiguity_check['is_contiguous']]['ts_id'])\n",
    "\n",
    "print(f\"Contiguous time series: {len(contiguous_ids)}\")\n",
    "print(f\"Non-contiguous time series (will be removed): {len(non_contiguous)}\")\n",
    "\n",
    "if len(non_contiguous) > 0:\n",
    "    print(f\"\\nSample non-contiguous series:\")\n",
    "    sample = non_contiguous.head(10)\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"  {row['ts_id']}: train ends {row['train_max_date'].date()}, val starts {row['val_min_date'].date()} (gap: {row['gap_days']} days)\")\n",
    "\n",
    "# Filter to only contiguous time series\n",
    "train_df = train_df[train_df['ts_id'].isin(contiguous_ids)]\n",
    "validation_df = validation_df[validation_df['ts_id'].isin(contiguous_ids)]\n",
    "\n",
    "print(f\"\\n=== Final Dataset ===\")\n",
    "print(f\"Training time series: {train_df['ts_id'].nunique()}\")\n",
    "print(f\"Validation time series: {validation_df['ts_id'].nunique()}\")\n",
    "print(f\"Training rows: {len(train_df)}\")\n",
    "print(f\"Validation rows: {len(validation_df)}\")\n",
    "\n",
    "# Convert timestamp to consistent date string format (no time component)\n",
    "train_df['timeStamp'] = pd.to_datetime(train_df['timeStamp']).dt.strftime('%Y-%m-%d')\n",
    "validation_df['timeStamp'] = pd.to_datetime(validation_df['timeStamp']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Verify no duplicates\n",
    "train_dupes = train_df.duplicated(subset=['ts_id', 'timeStamp']).sum()\n",
    "val_dupes = validation_df.duplicated(subset=['ts_id', 'timeStamp']).sum()\n",
    "print(f\"Train duplicates: {train_dupes}, Validation duplicates: {val_dupes}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('./data/training-mltable-folder', exist_ok=True)\n",
    "os.makedirs('./data/validation-mltable-folder', exist_ok=True)\n",
    "\n",
    "# Save as CSV (MLTable will reference these)\n",
    "train_df.to_csv('./data/training-mltable-folder/train.csv', index=False)\n",
    "validation_df.to_csv('./data/validation-mltable-folder/validation.csv', index=False)\n",
    "\n",
    "print(f\"\\nTraining data saved: {len(train_df)} rows\")\n",
    "print(f\"Validation data saved: {len(validation_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mltable_train = \"\"\"paths:\n",
    "  - file: ./train.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "mltable_val = \"\"\"paths:\n",
    "  - file: ./validation.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "with open('./data/training-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_train)\n",
    "    \n",
    "with open('./data/validation-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_val)\n",
    "\n",
    "print(\"MLTable files created:\")\n",
    "print(\"  - ./data/training-mltable-folder/MLTable\")\n",
    "print(\"  - ./data/validation-mltable-folder/MLTable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861505be",
   "metadata": {},
   "source": [
    "## 2.6 Upload Data to Azure Blob Storage\n",
    "\n",
    "Due to Azure Policy restrictions (SAS tokens disabled), data must be uploaded using Azure CLI with OAuth authentication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80df692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Azure Storage configuration\n",
    "STORAGE_ACCOUNT = \"mldemowkspwus02609576373\"\n",
    "CONTAINER = \"azureml-blobstore-cff56e3a-d016-4526-aa58-71c460675066\"\n",
    "\n",
    "def upload_to_blob(source_folder, destination_path):\n",
    "    \"\"\"Upload local folder to Azure Blob Storage using OAuth authentication.\"\"\"\n",
    "    # First, delete existing data to ensure fresh upload\n",
    "    delete_cmd = [\n",
    "        \"az\", \"storage\", \"blob\", \"delete-batch\",\n",
    "        \"--account-name\", STORAGE_ACCOUNT,\n",
    "        \"--source\", CONTAINER,\n",
    "        \"--pattern\", f\"{destination_path}/*\",\n",
    "        \"--auth-mode\", \"login\"\n",
    "    ]\n",
    "    print(f\"Cleaning {destination_path}...\")\n",
    "    subprocess.run(delete_cmd, capture_output=True, text=True)\n",
    "    \n",
    "    # Upload new data\n",
    "    upload_cmd = [\n",
    "        \"az\", \"storage\", \"blob\", \"upload-batch\",\n",
    "        \"--account-name\", STORAGE_ACCOUNT,\n",
    "        \"--destination\", CONTAINER,\n",
    "        \"--destination-path\", destination_path,\n",
    "        \"--source\", source_folder,\n",
    "        \"--auth-mode\", \"login\",\n",
    "        \"--overwrite\"\n",
    "    ]\n",
    "    print(f\"Uploading {source_folder} to {destination_path}...\")\n",
    "    result = subprocess.run(upload_cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úì Successfully uploaded to {destination_path}\")\n",
    "        print(f\"  Output: {result.stdout[:200] if result.stdout else 'OK'}\")\n",
    "    else:\n",
    "        print(f\"‚úó Upload failed: {result.stderr}\")\n",
    "    return result.returncode == 0\n",
    "\n",
    "# Upload training data to NEW path\n",
    "upload_to_blob(\"./data/training-mltable-folder\", \"retail-train-v2\")\n",
    "\n",
    "# Upload validation data to NEW path\n",
    "upload_to_blob(\"./data/validation-mltable-folder\", \"retail-val-v2\")\n",
    "\n",
    "print(\"\\nData upload complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc993c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, \n",
    "    path=\"azureml://datastores/workspaceblobstore_identity/paths/retail-train-v2\"\n",
    ")\n",
    "\n",
    "my_validation_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, \n",
    "    path=\"azureml://datastores/workspaceblobstore_identity/paths/retail-val-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929c4a9",
   "metadata": {},
   "source": [
    "# 3. Configure and Run AutoML Forecasting Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8836281",
   "metadata": {},
   "source": [
    "## 3.1 Job Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AutoML forecasting job with the related factory-function.\n",
    "forecasting_job = automl.forecasting(\n",
    "    experiment_name=\"sales-forecasting-v2\",\n",
    "    compute=\"teslat4-gpu-wus\",  \n",
    "    training_data=my_training_data_input,\n",
    "    validation_data=my_validation_data_input, \n",
    "    target_column_name=\"demand\",\n",
    "    primary_metric=\"NormalizedRootMeanSquaredError\",\n",
    "    enable_model_explainability=True,\n",
    "    tags={\"retail\": \"forecasting\"},\n",
    ")\n",
    "\n",
    "# Limits are all optional\n",
    "forecasting_job.set_limits(\n",
    "    timeout_minutes=600,\n",
    "    trial_timeout_minutes=20,\n",
    "    max_trials=5,\n",
    "    enable_early_termination=True,\n",
    ")\n",
    "\n",
    "# Specialized properties for Time Series Forecasting training\n",
    "forecasting_job.set_forecast_settings(\n",
    "    time_column_name=\"timeStamp\",\n",
    "    forecast_horizon=12,  # 12 weeks forecast\n",
    "    frequency=\"W-FRI\",  # pandas offset: W-FRI=weekly anchored on Friday (matches our data)\n",
    "    time_series_id_column_names=[\"ts_id\"],\n",
    "    short_series_handling_config=\"auto\",  # Auto-handle short/irregular series\n",
    "    target_lags=\"auto\",\n",
    ")\n",
    "\n",
    "# forecasting_job.set_training(blocked_training_algorithms=[\"ExtremeRandomTrees\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16457be8",
   "metadata": {},
   "source": [
    "## 3.2 Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40118475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the AutoML job\n",
    "returned_job = ml_client.jobs.create_or_update(forecasting_job)\n",
    "print(f\"Created job: {returned_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8129921",
   "metadata": {},
   "source": [
    "# 4. Register the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTE: The model requires Azure ML runtime which doesn't work on Apple Silicon.\n",
    "# Instead, we'll register the model and run batch inference in Azure ML.\n",
    "# ============================================================================\n",
    "\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "# Register the best model in Azure ML Model Registry\n",
    "print(\"Registering model in Azure ML...\")\n",
    "\n",
    "model = Model(\n",
    "    path=f\"azureml://jobs/{returned_job.name}/outputs/best_model\",\n",
    "    name=\"retail-sales-forecasting-model\",\n",
    "    description=\"AutoML time-series forecasting model for retail weekly sales\",\n",
    "    type=\"mlflow_model\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    registered_model = ml_client.models.create_or_update(model)\n",
    "    print(f\"‚úì Registered model: {registered_model.name}, version: {registered_model.version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model may already be registered: {e}\")\n",
    "    # Get existing model\n",
    "    registered_model = ml_client.models.get(name=\"retail-sales-forecasting-model\", version=\"latest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e1d0f",
   "metadata": {},
   "source": [
    "# 5. Deploy and Test the Model\n",
    "\n",
    "Deploy the model as a Managed Online Endpoint to test predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "905eae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint: retail-forecast-12022118\n",
      "This may take 5-10 minutes...\n",
      "‚úì Endpoint created: retail-forecast-12022118\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Create a Managed Online Endpoint\n",
    "# ============================================================================\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Create a unique endpoint name\n",
    "endpoint_name = f\"retail-forecast-{datetime.datetime.now().strftime('%m%d%H%M')}\"\n",
    "\n",
    "# Define the endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"Retail sales forecasting endpoint\",\n",
    "    auth_mode=\"key\"  # or \"aml_token\" for Azure AD auth\n",
    ")\n",
    "\n",
    "# Create the endpoint (this takes a few minutes)\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "print(\"This may take 5-10 minutes...\")\n",
    "\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "print(f\"‚úì Endpoint created: {endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8d03a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint retail-forecast-12022118 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: retail-sales-forecasting-model, version: 1\n",
      "Deploying model to endpoint...\n",
      "This may take 10-15 minutes...\n",
      "...............................................................................................‚úì Model deployed to: retail-forecast-12022118\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Deploy the Model to the Endpoint\n",
    "# ============================================================================\n",
    "\n",
    "# Get the registered model\n",
    "model = ml_client.models.get(name=\"retail-sales-forecasting-model\", version=\"1\")\n",
    "print(f\"Using model: {model.name}, version: {model.version}\")\n",
    "\n",
    "# Create the deployment\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=\"default\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",  # 4 cores, 14 GB RAM\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "# Deploy (this takes 5-15 minutes)\n",
    "print(f\"Deploying model to endpoint...\")\n",
    "print(\"This may take 10-15 minutes...\")\n",
    "\n",
    "ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
    "\n",
    "# Set the deployment to receive 100% of traffic\n",
    "endpoint.traffic = {\"default\": 100}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "\n",
    "print(f\"‚úì Model deployed to: {endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7422dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>...</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>StoreType_A</th>\n",
       "      <th>StoreType_B</th>\n",
       "      <th>StoreType_C</th>\n",
       "      <th>ts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>49.01</td>\n",
       "      <td>3.157</td>\n",
       "      <td>6277.39</td>\n",
       "      <td>21813.16</td>\n",
       "      <td>143.10</td>\n",
       "      <td>...</td>\n",
       "      <td>219.714258</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>48.53</td>\n",
       "      <td>3.261</td>\n",
       "      <td>5183.29</td>\n",
       "      <td>8025.87</td>\n",
       "      <td>42.24</td>\n",
       "      <td>...</td>\n",
       "      <td>219.892526</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-20</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>54.11</td>\n",
       "      <td>3.268</td>\n",
       "      <td>4139.87</td>\n",
       "      <td>2807.19</td>\n",
       "      <td>33.88</td>\n",
       "      <td>...</td>\n",
       "      <td>219.985689</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-27</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>54.26</td>\n",
       "      <td>3.290</td>\n",
       "      <td>1164.46</td>\n",
       "      <td>1082.74</td>\n",
       "      <td>44.00</td>\n",
       "      <td>...</td>\n",
       "      <td>220.078852</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-02-03</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>56.55</td>\n",
       "      <td>3.360</td>\n",
       "      <td>34577.06</td>\n",
       "      <td>3579.21</td>\n",
       "      <td>160.53</td>\n",
       "      <td>...</td>\n",
       "      <td>220.172015</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-02-10</td>\n",
       "      <td>True</td>\n",
       "      <td>151315</td>\n",
       "      <td>48.02</td>\n",
       "      <td>3.409</td>\n",
       "      <td>13925.06</td>\n",
       "      <td>6927.23</td>\n",
       "      <td>101.64</td>\n",
       "      <td>...</td>\n",
       "      <td>220.265178</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>45.32</td>\n",
       "      <td>3.510</td>\n",
       "      <td>9873.33</td>\n",
       "      <td>11062.27</td>\n",
       "      <td>9.80</td>\n",
       "      <td>...</td>\n",
       "      <td>220.425759</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-02-24</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>57.25</td>\n",
       "      <td>3.555</td>\n",
       "      <td>9349.61</td>\n",
       "      <td>7556.01</td>\n",
       "      <td>3.20</td>\n",
       "      <td>...</td>\n",
       "      <td>220.636902</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>60.96</td>\n",
       "      <td>3.630</td>\n",
       "      <td>15441.40</td>\n",
       "      <td>1569.00</td>\n",
       "      <td>10.80</td>\n",
       "      <td>...</td>\n",
       "      <td>220.848045</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-03-09</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>58.76</td>\n",
       "      <td>3.669</td>\n",
       "      <td>10331.04</td>\n",
       "      <td>151.88</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>221.059189</td>\n",
       "      <td>7.348</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept   timeStamp  IsHoliday    Size  Temperature  Fuel_Price  \\\n",
       "0      1     1  2012-01-06      False  151315        49.01       3.157   \n",
       "1      1     1  2012-01-13      False  151315        48.53       3.261   \n",
       "2      1     1  2012-01-20      False  151315        54.11       3.268   \n",
       "3      1     1  2012-01-27      False  151315        54.26       3.290   \n",
       "4      1     1  2012-02-03      False  151315        56.55       3.360   \n",
       "5      1     1  2012-02-10       True  151315        48.02       3.409   \n",
       "6      1     1  2012-02-17      False  151315        45.32       3.510   \n",
       "7      1     1  2012-02-24      False  151315        57.25       3.555   \n",
       "8      1     1  2012-03-02      False  151315        60.96       3.630   \n",
       "9      1     1  2012-03-09      False  151315        58.76       3.669   \n",
       "\n",
       "   MarkDown1  MarkDown2  MarkDown3  ...         CPI  Unemployment  Year  \\\n",
       "0    6277.39   21813.16     143.10  ...  219.714258         7.348  2012   \n",
       "1    5183.29    8025.87      42.24  ...  219.892526         7.348  2012   \n",
       "2    4139.87    2807.19      33.88  ...  219.985689         7.348  2012   \n",
       "3    1164.46    1082.74      44.00  ...  220.078852         7.348  2012   \n",
       "4   34577.06    3579.21     160.53  ...  220.172015         7.348  2012   \n",
       "5   13925.06    6927.23     101.64  ...  220.265178         7.348  2012   \n",
       "6    9873.33   11062.27       9.80  ...  220.425759         7.348  2012   \n",
       "7    9349.61    7556.01       3.20  ...  220.636902         7.348  2012   \n",
       "8   15441.40    1569.00      10.80  ...  220.848045         7.348  2012   \n",
       "9   10331.04     151.88       6.00  ...  221.059189         7.348  2012   \n",
       "\n",
       "   Month  Week  DayOfWeek  StoreType_A  StoreType_B  StoreType_C  ts_id  \n",
       "0      1     1          4         True        False        False    1_1  \n",
       "1      1     2          4         True        False        False    1_1  \n",
       "2      1     3          4         True        False        False    1_1  \n",
       "3      1     4          4         True        False        False    1_1  \n",
       "4      2     5          4         True        False        False    1_1  \n",
       "5      2     6          4         True        False        False    1_1  \n",
       "6      2     7          4         True        False        False    1_1  \n",
       "7      2     8          4         True        False        False    1_1  \n",
       "8      3     9          4         True        False        False    1_1  \n",
       "9      3    10          4         True        False        False    1_1  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Sample request saved to ./sample_request.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Test the Endpoint with Sample Data\n",
    "# ============================================================================\n",
    "import json\n",
    "\n",
    "# Load some validation data for testing\n",
    "test_df = pd.read_csv('./data/validation-mltable-folder/validation.csv')\n",
    "\n",
    "# Take a small sample (first 10 rows from one time series)\n",
    "sample = test_df[test_df['ts_id'] == '1_1'].head(10).copy()\n",
    "\n",
    "# Remove the target column (demand) - we want to predict this\n",
    "sample_input = sample.drop(columns=['demand'])\n",
    "\n",
    "print(\"Sample input data:\")\n",
    "display(sample_input)\n",
    "\n",
    "# Convert to the format expected by the endpoint\n",
    "# MLflow models expect a pandas-split format or records format\n",
    "input_data = {\n",
    "    \"input_data\": sample_input.to_dict(orient=\"records\")\n",
    "}\n",
    "\n",
    "# Save to a temporary file for the invoke\n",
    "with open(\"./sample_request.json\", \"w\") as f:\n",
    "    json.dump(input_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Sample request saved to ./sample_request.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed95aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling endpoint: retail-forecast-12022118\n"
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "(None) An unexpected error occurred in scoring script. Check the logs for more info.\nCode: None\nMessage: An unexpected error occurred in scoring script. Check the logs for more info.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Step 4: Invoke the Endpoint to Get Predictions\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Invoke the endpoint\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mml_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43monline_endpoints\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./sample_request.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Parse the response\u001b[39;00m\n\u001b[32m     14\u001b[39m predictions = json.loads(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_telemetry/activity.py:288\u001b[39m, in \u001b[36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(ACTIVITY_SPAN):\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[32m    286\u001b[39m             logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[32m    287\u001b[39m         ):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[33m\"\u001b[39m\u001b[33mpackage_logger\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/operations/_online_endpoint_operations.py:366\u001b[39m, in \u001b[36mOnlineEndpointOperations.invoke\u001b[39m\u001b[34m(self, endpoint_name, request_file, deployment_name, input_data, params_override, local, **kwargs)\u001b[39m\n\u001b[32m    363\u001b[39m     headers[EndpointInvokeFields.MODEL_DEPLOYMENT] = deployment_name\n\u001b[32m    365\u001b[39m response = \u001b[38;5;28mself\u001b[39m._requests_pipeline.post(endpoint.properties.scoring_uri, json=data, headers=headers)\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m \u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(response.text())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_utils/_endpoint_utils.py:133\u001b[39m, in \u001b[36mvalidate_response\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    125\u001b[39m failure_msg = r_json.get(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m, response)\n\u001b[32m    126\u001b[39m error_map = {\n\u001b[32m    127\u001b[39m     \u001b[32m401\u001b[39m: ClientAuthenticationError,\n\u001b[32m    128\u001b[39m     \u001b[32m404\u001b[39m: ResourceNotFoundError,\n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m     \u001b[32m424\u001b[39m: HttpResponseError,\n\u001b[32m    132\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response, message=failure_msg, error_format=ARMErrorFormat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/core/exceptions.py:163\u001b[39m, in \u001b[36mmap_error\u001b[39m\u001b[34m(status_code, response, error_map)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    162\u001b[39m error = error_type(response=response)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[31mHttpResponseError\u001b[39m: (None) An unexpected error occurred in scoring script. Check the logs for more info.\nCode: None\nMessage: An unexpected error occurred in scoring script. Check the logs for more info."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Invoke the Endpoint to Get Predictions\n",
    "# ============================================================================\n",
    "\n",
    "# Invoke the endpoint\n",
    "print(f\"Calling endpoint: {endpoint_name}\")\n",
    "\n",
    "response = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=endpoint_name,\n",
    "    request_file=\"./sample_request.json\"\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "predictions = json.loads(response)\n",
    "print(\"\\nüìä PREDICTIONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Add predictions to sample data for comparison\n",
    "sample['predicted_demand'] = predictions\n",
    "sample['error'] = sample['demand'] - sample['predicted_demand']\n",
    "\n",
    "display(sample[['Store', 'Dept', 'timeStamp', 'demand', 'predicted_demand', 'error']])\n",
    "\n",
    "# Calculate simple metrics\n",
    "mae = abs(sample['error']).mean()\n",
    "print(f\"\\nMean Absolute Error (sample): ${mae:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Clean Up (Delete Endpoint when done)\n",
    "# ============================================================================\n",
    "# \n",
    "# ‚ö†Ô∏è  IMPORTANT: Endpoints cost money while running!\n",
    "# Delete the endpoint when you're done testing.\n",
    "#\n",
    "# Uncomment the lines below to delete:\n",
    "\n",
    "# print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "# ml_client.online_endpoints.begin_delete(name=endpoint_name).result()\n",
    "# print(\"‚úì Endpoint deleted\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  Remember to delete the endpoint when done to avoid charges!\")\n",
    "print(f\"   Endpoint name: {endpoint_name}\")\n",
    "print(f\"   Delete in Azure Portal or uncomment the code above.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
