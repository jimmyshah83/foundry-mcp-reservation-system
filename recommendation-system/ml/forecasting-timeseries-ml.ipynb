{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6d4eda",
   "metadata": {},
   "source": [
    "# AutoML: Train \"the best\" Time-Series Forecasting model for Retail Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f23d42",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c55927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress OpenTelemetry warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Overriding of current\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Attempting to instrument\")\n",
    "\n",
    "# Suppress Azure SDK telemetry logging\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "credential = AzureCliCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    subscription_id = \"57123c17-af1a-4ec2-9494-a214fb148bf4\"\n",
    "    resource_group = \"admin-rg\"\n",
    "    workspace = \"ml-demo-wksp-wus-01\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "except Exception as ex:\n",
    "    print(\"Ex:\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection\n",
    "ws = ml_client.workspaces.get(ml_client.workspace_name)\n",
    "print(f\"Connected to: {ws.name} ({ws.location})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ebeb9",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "Using [Retail data analytics](https://www.kaggle.com/datasets/manjeetsingh/retaildataset) - weekly sales by store and department."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e5c30",
   "metadata": {},
   "source": [
    "## 2.1 Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a98900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "stores_df = pd.read_csv('../dataset/stores data-set.csv')\n",
    "features_df = pd.read_csv('../dataset/Features data set.csv')\n",
    "sales_df = pd.read_csv('../dataset/sales data-set.csv')\n",
    "\n",
    "# Quick exploration\n",
    "print(f\"Stores: {stores_df.shape}\")\n",
    "print(f\"Features: {features_df.shape}\")\n",
    "print(f\"Sales: {sales_df.shape}\")\n",
    "\n",
    "print(\"\\n--- Stores Data ---\")\n",
    "display(stores_df.head())\n",
    "\n",
    "print(\"\\n--- Features Data ---\")\n",
    "display(features_df.head())\n",
    "\n",
    "print(\"\\n--- Sales Data ---\")\n",
    "display(sales_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffc88b",
   "metadata": {},
   "source": [
    "## 2.2 Merge Datasets\n",
    "Merge sales with stores (on Store) and then with features (on Store and Date).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales with stores (on Store)\n",
    "merged_df = sales_df.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "# Merge with features (on Store and Date)\n",
    "merged_df = merged_df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
    "\n",
    "# Drop duplicate IsHoliday column from features\n",
    "merged_df = merged_df.drop(columns=['IsHoliday_feat'])\n",
    "\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "print(f\"\\nColumns: {merged_df.columns.tolist()}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e3f01",
   "metadata": {},
   "source": [
    "## 2.3 Feature Engineering\n",
    "Create new features from date, handle missing MarkDown values, and encode categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to datetime (format is dd/mm/yyyy)\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "\n",
    "# Extract date features\n",
    "merged_df['Year'] = merged_df['Date'].dt.year\n",
    "merged_df['Month'] = merged_df['Date'].dt.month\n",
    "merged_df['Week'] = merged_df['Date'].dt.isocalendar().week\n",
    "merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "# Handle missing MarkDown values (only available after Nov 2011)\n",
    "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "merged_df[markdown_cols] = merged_df[markdown_cols].fillna(0)\n",
    "\n",
    "# Encode categorical: Store Type (A, B, C)\n",
    "if 'Type' in merged_df.columns:\n",
    "    merged_df = pd.get_dummies(merged_df, columns=['Type'], prefix='StoreType')\n",
    "\n",
    "print(f\"Feature engineered dataset: {merged_df.shape}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0205d",
   "metadata": {},
   "source": [
    "## 2.4 Time-Based Train/Validation Split\n",
    "\n",
    "Split data chronologically: train on data before 2012, validate on 2012 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ab93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "dupes = merged_df.groupby(['Store', 'Dept', 'Date']).size()\n",
    "print(f\"Duplicate combinations: {(dupes > 1).sum()}\")\n",
    "\n",
    "# Aggregate duplicates: sum Weekly_Sales, take first for other columns\n",
    "agg_funcs = {col: 'first' for col in merged_df.columns if col not in ['Store', 'Dept', 'Date']}\n",
    "agg_funcs['Weekly_Sales'] = 'sum'\n",
    "merged_df = merged_df.groupby(['Store', 'Dept', 'Date'], as_index=False).agg(agg_funcs)\n",
    "print(f\"After deduplication: {merged_df.shape}\")\n",
    "\n",
    "# Sort by date\n",
    "merged_df = merged_df.sort_values(['Store', 'Dept', 'Date'])\n",
    "\n",
    "# Time-based split: train on data before 2012, validate on 2012\n",
    "train_df = merged_df[merged_df['Year'] < 2012].copy()\n",
    "validation_df = merged_df[merged_df['Year'] >= 2012].copy()\n",
    "\n",
    "print(f\"\\nTraining set: {train_df.shape}\")\n",
    "print(f\"Validation set: {validation_df.shape}\")\n",
    "print(f\"Train date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
    "print(f\"Validation date range: {validation_df['Date'].min()} to {validation_df['Date'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00bca9",
   "metadata": {},
   "source": [
    "## 2.5 Prepare Data for Azure ML AutoML\n",
    "\n",
    "Rename columns to match AutoML expectations and save as MLTable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Rename columns for AutoML compatibility\n",
    "train_df = train_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "validation_df = validation_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "\n",
    "# Create single time series ID column (before converting dates to strings)\n",
    "train_df['ts_id'] = train_df['Store'].astype(str) + '_' + train_df['Dept'].astype(str)\n",
    "validation_df['ts_id'] = validation_df['Store'].astype(str) + '_' + validation_df['Dept'].astype(str)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Filter to common time series IDs (both train and validation must have same IDs)\n",
    "# ============================================================================\n",
    "train_ts_ids = set(train_df['ts_id'].unique())\n",
    "val_ts_ids = set(validation_df['ts_id'].unique())\n",
    "\n",
    "val_only_ids = val_ts_ids - train_ts_ids\n",
    "train_only_ids = train_ts_ids - val_ts_ids\n",
    "common_ids = train_ts_ids & val_ts_ids\n",
    "\n",
    "print(f\"=== Step 1: Filter to common time series ===\")\n",
    "print(f\"Time series in training only: {len(train_only_ids)}\")\n",
    "print(f\"Time series in validation only: {len(val_only_ids)}\")\n",
    "print(f\"Common time series: {len(common_ids)}\")\n",
    "\n",
    "# Keep only common IDs\n",
    "train_df = train_df[train_df['ts_id'].isin(common_ids)]\n",
    "validation_df = validation_df[validation_df['ts_id'].isin(common_ids)]\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Check contiguity - validation must start right after training ends\n",
    "# For weekly data, the gap should be exactly 7 days\n",
    "# ============================================================================\n",
    "print(f\"\\n=== Step 2: Check contiguity (no gaps between train and validation) ===\")\n",
    "\n",
    "# Get max date per ts_id in training\n",
    "train_max_dates = train_df.groupby('ts_id')['timeStamp'].max().reset_index()\n",
    "train_max_dates.columns = ['ts_id', 'train_max_date']\n",
    "\n",
    "# Get min date per ts_id in validation\n",
    "val_min_dates = validation_df.groupby('ts_id')['timeStamp'].min().reset_index()\n",
    "val_min_dates.columns = ['ts_id', 'val_min_date']\n",
    "\n",
    "# Merge to compare\n",
    "contiguity_check = train_max_dates.merge(val_min_dates, on='ts_id')\n",
    "contiguity_check['train_max_date'] = pd.to_datetime(contiguity_check['train_max_date'])\n",
    "contiguity_check['val_min_date'] = pd.to_datetime(contiguity_check['val_min_date'])\n",
    "contiguity_check['gap_days'] = (contiguity_check['val_min_date'] - contiguity_check['train_max_date']).dt.days\n",
    "\n",
    "# For weekly data, gap should be 7 days (next week)\n",
    "# Allow some flexibility: 6-8 days is acceptable\n",
    "contiguity_check['is_contiguous'] = contiguity_check['gap_days'].between(6, 8)\n",
    "\n",
    "non_contiguous = contiguity_check[~contiguity_check['is_contiguous']]\n",
    "contiguous_ids = set(contiguity_check[contiguity_check['is_contiguous']]['ts_id'])\n",
    "\n",
    "print(f\"Contiguous time series: {len(contiguous_ids)}\")\n",
    "print(f\"Non-contiguous time series (will be removed): {len(non_contiguous)}\")\n",
    "\n",
    "if len(non_contiguous) > 0:\n",
    "    print(f\"\\nSample non-contiguous series:\")\n",
    "    sample = non_contiguous.head(10)\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"  {row['ts_id']}: train ends {row['train_max_date'].date()}, val starts {row['val_min_date'].date()} (gap: {row['gap_days']} days)\")\n",
    "\n",
    "# Filter to only contiguous time series\n",
    "train_df = train_df[train_df['ts_id'].isin(contiguous_ids)]\n",
    "validation_df = validation_df[validation_df['ts_id'].isin(contiguous_ids)]\n",
    "\n",
    "print(f\"\\n=== Final Dataset ===\")\n",
    "print(f\"Training time series: {train_df['ts_id'].nunique()}\")\n",
    "print(f\"Validation time series: {validation_df['ts_id'].nunique()}\")\n",
    "print(f\"Training rows: {len(train_df)}\")\n",
    "print(f\"Validation rows: {len(validation_df)}\")\n",
    "\n",
    "# Convert timestamp to consistent date string format (no time component)\n",
    "train_df['timeStamp'] = pd.to_datetime(train_df['timeStamp']).dt.strftime('%Y-%m-%d')\n",
    "validation_df['timeStamp'] = pd.to_datetime(validation_df['timeStamp']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Verify no duplicates\n",
    "train_dupes = train_df.duplicated(subset=['ts_id', 'timeStamp']).sum()\n",
    "val_dupes = validation_df.duplicated(subset=['ts_id', 'timeStamp']).sum()\n",
    "print(f\"Train duplicates: {train_dupes}, Validation duplicates: {val_dupes}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('./data/training-mltable-folder', exist_ok=True)\n",
    "os.makedirs('./data/validation-mltable-folder', exist_ok=True)\n",
    "\n",
    "# Save as CSV (MLTable will reference these)\n",
    "train_df.to_csv('./data/training-mltable-folder/train.csv', index=False)\n",
    "validation_df.to_csv('./data/validation-mltable-folder/validation.csv', index=False)\n",
    "\n",
    "print(f\"\\nTraining data saved: {len(train_df)} rows\")\n",
    "print(f\"Validation data saved: {len(validation_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mltable_train = \"\"\"paths:\n",
    "  - file: ./train.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "mltable_val = \"\"\"paths:\n",
    "  - file: ./validation.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "with open('./data/training-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_train)\n",
    "    \n",
    "with open('./data/validation-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_val)\n",
    "\n",
    "print(\"MLTable files created:\")\n",
    "print(\"  - ./data/training-mltable-folder/MLTable\")\n",
    "print(\"  - ./data/validation-mltable-folder/MLTable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861505be",
   "metadata": {},
   "source": [
    "## 2.6 Upload Data to Azure Blob Storage\n",
    "\n",
    "Due to Azure Policy restrictions (SAS tokens disabled), data must be uploaded using Azure CLI with OAuth authentication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80df692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Azure Storage configuration\n",
    "STORAGE_ACCOUNT = \"mldemowkspwus02609576373\"\n",
    "CONTAINER = \"azureml-blobstore-cff56e3a-d016-4526-aa58-71c460675066\"\n",
    "\n",
    "def upload_to_blob(source_folder, destination_path):\n",
    "    \"\"\"Upload local folder to Azure Blob Storage using OAuth authentication.\"\"\"\n",
    "    # First, delete existing data to ensure fresh upload\n",
    "    delete_cmd = [\n",
    "        \"az\", \"storage\", \"blob\", \"delete-batch\",\n",
    "        \"--account-name\", STORAGE_ACCOUNT,\n",
    "        \"--source\", CONTAINER,\n",
    "        \"--pattern\", f\"{destination_path}/*\",\n",
    "        \"--auth-mode\", \"login\"\n",
    "    ]\n",
    "    print(f\"Cleaning {destination_path}...\")\n",
    "    subprocess.run(delete_cmd, capture_output=True, text=True)\n",
    "    \n",
    "    # Upload new data\n",
    "    upload_cmd = [\n",
    "        \"az\", \"storage\", \"blob\", \"upload-batch\",\n",
    "        \"--account-name\", STORAGE_ACCOUNT,\n",
    "        \"--destination\", CONTAINER,\n",
    "        \"--destination-path\", destination_path,\n",
    "        \"--source\", source_folder,\n",
    "        \"--auth-mode\", \"login\",\n",
    "        \"--overwrite\"\n",
    "    ]\n",
    "    print(f\"Uploading {source_folder} to {destination_path}...\")\n",
    "    result = subprocess.run(upload_cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ Successfully uploaded to {destination_path}\")\n",
    "        print(f\"  Output: {result.stdout[:200] if result.stdout else 'OK'}\")\n",
    "    else:\n",
    "        print(f\"✗ Upload failed: {result.stderr}\")\n",
    "    return result.returncode == 0\n",
    "\n",
    "# Upload training data to NEW path\n",
    "upload_to_blob(\"./data/training-mltable-folder\", \"retail-train-v2\")\n",
    "\n",
    "# Upload validation data to NEW path\n",
    "upload_to_blob(\"./data/validation-mltable-folder\", \"retail-val-v2\")\n",
    "\n",
    "print(\"\\nData upload complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc993c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, \n",
    "    path=\"azureml://datastores/workspaceblobstore_identity/paths/retail-train-v2\"\n",
    ")\n",
    "\n",
    "my_validation_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, \n",
    "    path=\"azureml://datastores/workspaceblobstore_identity/paths/retail-val-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929c4a9",
   "metadata": {},
   "source": [
    "# 3. Configure and Run AutoML Forecasting Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8836281",
   "metadata": {},
   "source": [
    "## 3.1 Job Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AutoML forecasting job with the related factory-function.\n",
    "forecasting_job = automl.forecasting(\n",
    "    experiment_name=\"sales-forecasting-v2\",\n",
    "    compute=\"teslat4-gpu-wus\",  \n",
    "    training_data=my_training_data_input,\n",
    "    validation_data=my_validation_data_input, \n",
    "    target_column_name=\"demand\",\n",
    "    primary_metric=\"NormalizedRootMeanSquaredError\",\n",
    "    enable_model_explainability=True,\n",
    "    tags={\"retail\": \"forecasting\"},\n",
    ")\n",
    "\n",
    "# Limits are all optional\n",
    "forecasting_job.set_limits(\n",
    "    timeout_minutes=600,\n",
    "    trial_timeout_minutes=20,\n",
    "    max_trials=5,\n",
    "    enable_early_termination=True,\n",
    ")\n",
    "\n",
    "# Specialized properties for Time Series Forecasting training\n",
    "forecasting_job.set_forecast_settings(\n",
    "    time_column_name=\"timeStamp\",\n",
    "    forecast_horizon=12,  # 12 weeks forecast\n",
    "    frequency=\"W-FRI\",  # pandas offset: W-FRI=weekly anchored on Friday (matches our data)\n",
    "    time_series_id_column_names=[\"ts_id\"],\n",
    "    short_series_handling_config=\"auto\",  # Auto-handle short/irregular series\n",
    "    target_lags=\"auto\",\n",
    ")\n",
    "\n",
    "# forecasting_job.set_training(blocked_training_algorithms=[\"ExtremeRandomTrees\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16457be8",
   "metadata": {},
   "source": [
    "## 3.2 Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40118475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the AutoML job\n",
    "returned_job = ml_client.jobs.create_or_update(forecasting_job)\n",
    "print(f\"Created job: {returned_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8129921",
   "metadata": {},
   "source": [
    "# 4. Get Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08e102",
   "metadata": {},
   "source": [
    "## 4.1 Load Model and Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b922eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model in Azure ML...\n",
      "✓ Registered model: retail-sales-forecasting-model, version: 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NOTE: The model requires Azure ML runtime which doesn't work on Apple Silicon.\n",
    "# Instead, we'll register the model and run batch inference in Azure ML.\n",
    "# ============================================================================\n",
    "\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "# Register the best model in Azure ML Model Registry\n",
    "print(\"Registering model in Azure ML...\")\n",
    "\n",
    "model = Model(\n",
    "    path=f\"azureml://jobs/{returned_job.name}/outputs/best_model\",\n",
    "    name=\"retail-sales-forecasting-model\",\n",
    "    description=\"AutoML time-series forecasting model for retail weekly sales\",\n",
    "    type=\"mlflow_model\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    registered_model = ml_client.models.create_or_update(model)\n",
    "    print(f\"✓ Registered model: {registered_model.name}, version: {registered_model.version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model may already be registered: {e}\")\n",
    "    # Get existing model\n",
    "    registered_model = ml_client.models.get(name=\"retail-sales-forecasting-model\", version=\"latest\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
