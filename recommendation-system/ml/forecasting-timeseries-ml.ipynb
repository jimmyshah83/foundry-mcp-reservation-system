{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6d4eda",
   "metadata": {},
   "source": [
    "# AutoML: Train \"the best\" Time-Series Forecasting model for Retail Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f23d42",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c55927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress OpenTelemetry warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Overriding of current\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Attempting to instrument\")\n",
    "\n",
    "# Suppress Azure SDK telemetry logging\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea94c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac07cb",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c9e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    subscription_id = \"57123c17-af1a-4ec2-9494-a214fb148bf4\"\n",
    "    resource_group = \"admin-rg\"\n",
    "    workspace = \"ml-demo-wksp-wus-01\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "except Exception as ex:\n",
    "    print(\"Ex:\", ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7073162",
   "metadata": {},
   "source": [
    "### Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c350bf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Workspace': 'ml-demo-wksp-wus-01',\n",
       " 'Subscription ID': '57123c17-af1a-4ec2-9494-a214fb148bf4',\n",
       " 'Resource Group': 'admin-rg',\n",
       " 'Location': 'westus'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = ml_client.workspace_name\n",
    "output[\"Subscription ID\"] = ml_client.subscription_id\n",
    "output[\"Resource Group\"] = workspace.resource_group\n",
    "output[\"Location\"] = workspace.location\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ebeb9",
   "metadata": {},
   "source": [
    "# 2. Data\n",
    "\n",
    "We will use [Retail data analytics] (https://www.kaggle.com/datasets/manjeetsingh/retaildataset) for model training. \n",
    "The data is stored in a tabular format and includes sales by store and department on a daily basis. \n",
    "\n",
    "With Azure Machine Learning MLTables you can keep a single copy of data in your storage, easily access data during model training, share data and collaborate with other users. \n",
    "Below, we will upload the data by creating an MLTable to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e5c30",
   "metadata": {},
   "source": [
    "## 2.1 Load and Explore Datasets\n",
    "\n",
    "Load the three retail datasets: stores, features, and sales data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a98900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stores: (45, 3)\n",
      "Features: (8190, 12)\n",
      "Sales: (421570, 5)\n",
      "\n",
      "--- Stores Data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>202307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "      <td>37392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>205863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B</td>\n",
       "      <td>34875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store Type    Size\n",
       "0      1    A  151315\n",
       "1      2    A  202307\n",
       "2      3    B   37392\n",
       "3      4    A  205863\n",
       "4      5    B   34875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Features Data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>05/02/2010</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12/02/2010</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19/02/2010</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26/02/2010</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>05/03/2010</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
       "0      1  05/02/2010        42.31       2.572        NaN        NaN   \n",
       "1      1  12/02/2010        38.51       2.548        NaN        NaN   \n",
       "2      1  19/02/2010        39.93       2.514        NaN        NaN   \n",
       "3      1  26/02/2010        46.63       2.561        NaN        NaN   \n",
       "4      1  05/03/2010        46.50       2.625        NaN        NaN   \n",
       "\n",
       "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
       "0        NaN        NaN        NaN  211.096358         8.106      False  \n",
       "1        NaN        NaN        NaN  211.242170         8.106       True  \n",
       "2        NaN        NaN        NaN  211.289143         8.106      False  \n",
       "3        NaN        NaN        NaN  211.319643         8.106      False  \n",
       "4        NaN        NaN        NaN  211.350143         8.106      False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sales Data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>05/02/2010</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12/02/2010</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19/02/2010</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26/02/2010</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>05/03/2010</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
       "0      1     1  05/02/2010      24924.50      False\n",
       "1      1     1  12/02/2010      46039.49       True\n",
       "2      1     1  19/02/2010      41595.55      False\n",
       "3      1     1  26/02/2010      19403.54      False\n",
       "4      1     1  05/03/2010      21827.90      False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "stores_df = pd.read_csv('../dataset/stores data-set.csv')\n",
    "features_df = pd.read_csv('../dataset/Features data set.csv')\n",
    "sales_df = pd.read_csv('../dataset/sales data-set.csv')\n",
    "\n",
    "# Quick exploration\n",
    "print(f\"Stores: {stores_df.shape}\")\n",
    "print(f\"Features: {features_df.shape}\")\n",
    "print(f\"Sales: {sales_df.shape}\")\n",
    "\n",
    "print(\"\\n--- Stores Data ---\")\n",
    "display(stores_df.head())\n",
    "\n",
    "print(\"\\n--- Features Data ---\")\n",
    "display(features_df.head())\n",
    "\n",
    "print(\"\\n--- Sales Data ---\")\n",
    "display(sales_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffc88b",
   "metadata": {},
   "source": [
    "## 2.2 Merge Datasets\n",
    "\n",
    "Merge sales with stores (on Store) and then with features (on Store and Date).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec8e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (421570, 16)\n",
      "\n",
      "Columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>05/02/2010</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12/02/2010</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19/02/2010</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26/02/2010</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>05/03/2010</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  Weekly_Sales  IsHoliday Type    Size  Temperature  \\\n",
       "0      1     1  05/02/2010      24924.50      False    A  151315        42.31   \n",
       "1      1     1  12/02/2010      46039.49       True    A  151315        38.51   \n",
       "2      1     1  19/02/2010      41595.55      False    A  151315        39.93   \n",
       "3      1     1  26/02/2010      19403.54      False    A  151315        46.63   \n",
       "4      1     1  05/03/2010      21827.90      False    A  151315        46.50   \n",
       "\n",
       "   Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  \\\n",
       "0       2.572        NaN        NaN        NaN        NaN        NaN   \n",
       "1       2.548        NaN        NaN        NaN        NaN        NaN   \n",
       "2       2.514        NaN        NaN        NaN        NaN        NaN   \n",
       "3       2.561        NaN        NaN        NaN        NaN        NaN   \n",
       "4       2.625        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "          CPI  Unemployment  \n",
       "0  211.096358         8.106  \n",
       "1  211.242170         8.106  \n",
       "2  211.289143         8.106  \n",
       "3  211.319643         8.106  \n",
       "4  211.350143         8.106  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge sales with stores (on Store)\n",
    "merged_df = sales_df.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "# Merge with features (on Store and Date)\n",
    "merged_df = merged_df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
    "\n",
    "# Drop duplicate IsHoliday column from features\n",
    "merged_df = merged_df.drop(columns=['IsHoliday_feat'])\n",
    "\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "print(f\"\\nColumns: {merged_df.columns.tolist()}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e3f01",
   "metadata": {},
   "source": [
    "## 2.3 Feature Engineering\n",
    "\n",
    "Create new features from date, handle missing MarkDown values, and encode categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d2b1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineered dataset shape: (421570, 23)\n",
      "\n",
      "New columns added:\n",
      "  - Date features: Year, Month, Week, DayOfWeek\n",
      "  - Store type encoded: StoreType_A, StoreType_B, StoreType_C\n",
      "  - Time series ID: ts_id\n",
      "\n",
      "Missing values in MarkDown columns (after fill):\n",
      "MarkDown1    0\n",
      "MarkDown2    0\n",
      "MarkDown3    0\n",
      "MarkDown4    0\n",
      "MarkDown5    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>...</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>StoreType_A</th>\n",
       "      <th>StoreType_B</th>\n",
       "      <th>StoreType_C</th>\n",
       "      <th>ts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>151315</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>151315</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept       Date  Weekly_Sales  IsHoliday    Size  Temperature  \\\n",
       "0      1     1 2010-02-05      24924.50      False  151315        42.31   \n",
       "1      1     1 2010-02-12      46039.49       True  151315        38.51   \n",
       "2      1     1 2010-02-19      41595.55      False  151315        39.93   \n",
       "3      1     1 2010-02-26      19403.54      False  151315        46.63   \n",
       "4      1     1 2010-03-05      21827.90      False  151315        46.50   \n",
       "\n",
       "   Fuel_Price  MarkDown1  MarkDown2  ...         CPI  Unemployment  Year  \\\n",
       "0       2.572        0.0        0.0  ...  211.096358         8.106  2010   \n",
       "1       2.548        0.0        0.0  ...  211.242170         8.106  2010   \n",
       "2       2.514        0.0        0.0  ...  211.289143         8.106  2010   \n",
       "3       2.561        0.0        0.0  ...  211.319643         8.106  2010   \n",
       "4       2.625        0.0        0.0  ...  211.350143         8.106  2010   \n",
       "\n",
       "   Month  Week  DayOfWeek  StoreType_A  StoreType_B  StoreType_C  ts_id  \n",
       "0      2     5          4         True        False        False    1_1  \n",
       "1      2     6          4         True        False        False    1_1  \n",
       "2      2     7          4         True        False        False    1_1  \n",
       "3      2     8          4         True        False        False    1_1  \n",
       "4      3     9          4         True        False        False    1_1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Date to datetime (format is dd/mm/yyyy)\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "\n",
    "# Extract date features\n",
    "merged_df['Year'] = merged_df['Date'].dt.year\n",
    "merged_df['Month'] = merged_df['Date'].dt.month\n",
    "merged_df['Week'] = merged_df['Date'].dt.isocalendar().week\n",
    "merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "# Handle missing MarkDown values (only available after Nov 2011)\n",
    "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "merged_df[markdown_cols] = merged_df[markdown_cols].fillna(0)\n",
    "\n",
    "# Encode categorical: Store Type (A, B, C)\n",
    "merged_df = pd.get_dummies(merged_df, columns=['Type'], prefix='StoreType')\n",
    "\n",
    "# Create time series identifier\n",
    "merged_df['ts_id'] = merged_df['Store'].astype(str) + '_' + merged_df['Dept'].astype(str)\n",
    "\n",
    "print(f\"Feature engineered dataset shape: {merged_df.shape}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(f\"  - Date features: Year, Month, Week, DayOfWeek\")\n",
    "print(f\"  - Store type encoded: StoreType_A, StoreType_B, StoreType_C\")\n",
    "print(f\"  - Time series ID: ts_id\")\n",
    "print(f\"\\nMissing values in MarkDown columns (after fill):\")\n",
    "print(merged_df[markdown_cols].isnull().sum())\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0205d",
   "metadata": {},
   "source": [
    "## 2.4 Time-Based Train/Validation Split\n",
    "\n",
    "Split data chronologically: train on data before 2012, validate on 2012 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5ab93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (294132, 23)\n",
      "Validation set: (127438, 23)\n",
      "\n",
      "Train date range: 2010-02-05 00:00:00 to 2011-12-30 00:00:00\n",
      "Validation date range: 2012-01-06 00:00:00 to 2012-10-26 00:00:00\n",
      "\n",
      "Train/Validation split ratio: 69.8% / 30.2%\n"
     ]
    }
   ],
   "source": [
    "# Sort by date\n",
    "merged_df = merged_df.sort_values(['Store', 'Dept', 'Date'])\n",
    "\n",
    "# Time-based split: train on data before 2012, validate on 2012\n",
    "train_df = merged_df[merged_df['Year'] < 2012].copy()\n",
    "validation_df = merged_df[merged_df['Year'] >= 2012].copy()\n",
    "\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Validation set: {validation_df.shape}\")\n",
    "print(f\"\\nTrain date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
    "print(f\"Validation date range: {validation_df['Date'].min()} to {validation_df['Date'].max()}\")\n",
    "print(f\"\\nTrain/Validation split ratio: {len(train_df)/(len(train_df)+len(validation_df))*100:.1f}% / {len(validation_df)/(len(train_df)+len(validation_df))*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00bca9",
   "metadata": {},
   "source": [
    "## 2.5 Prepare Data for Azure ML AutoML\n",
    "\n",
    "Rename columns to match AutoML expectations and save as MLTable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09c661c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to: ./data/training-mltable-folder/train.csv\n",
      "Validation data saved to: ./data/validation-mltable-folder/validation.csv\n",
      "\n",
      "Columns in final datasets:\n",
      "['Store', 'Dept', 'timeStamp', 'demand', 'IsHoliday', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Year', 'Month', 'Week', 'DayOfWeek', 'StoreType_A', 'StoreType_B', 'StoreType_C', 'ts_id']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Rename columns for AutoML compatibility\n",
    "train_df = train_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "validation_df = validation_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('./data/training-mltable-folder', exist_ok=True)\n",
    "os.makedirs('./data/validation-mltable-folder', exist_ok=True)\n",
    "\n",
    "# Save as CSV (MLTable will reference these)\n",
    "train_df.to_csv('./data/training-mltable-folder/train.csv', index=False)\n",
    "validation_df.to_csv('./data/validation-mltable-folder/validation.csv', index=False)\n",
    "\n",
    "print(f\"Training data saved to: ./data/training-mltable-folder/train.csv\")\n",
    "print(f\"Validation data saved to: ./data/validation-mltable-folder/validation.csv\")\n",
    "print(f\"\\nColumns in final datasets:\")\n",
    "print(train_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6ea09bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLTable files created:\n",
      "  - ./data/training-mltable-folder/MLTable\n",
      "  - ./data/validation-mltable-folder/MLTable\n"
     ]
    }
   ],
   "source": [
    "# Create MLTable YAML files for Azure ML\n",
    "\n",
    "mltable_train = \"\"\"paths:\n",
    "  - file: ./train.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "mltable_val = \"\"\"paths:\n",
    "  - file: ./validation.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "with open('./data/training-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_train)\n",
    "    \n",
    "with open('./data/validation-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_val)\n",
    "\n",
    "print(\"MLTable files created:\")\n",
    "print(\"  - ./data/training-mltable-folder/MLTable\")\n",
    "print(\"  - ./data/validation-mltable-folder/MLTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecc993c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training MLTable defined locally, with local data to be uploaded\n",
    "my_training_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, path=\"./data/training-mltable-folder\"\n",
    ")\n",
    "\n",
    "# Training MLTable defined locally, with local data to be uploaded\n",
    "my_validation_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, path=\"./data/validation-mltable-folder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929c4a9",
   "metadata": {},
   "source": [
    "# 3. Configure and run the AutoML Forecasting training job\n",
    "In this section we will configure and run the AutoML job, for training the model.\n",
    "\n",
    "## 3.1 Configure the job through the forecasting() factory function\n",
    "\n",
    "### forecasting() function parameters:\n",
    "\n",
    "The `forecasting()` factory function allows user to configure AutoML for the forecasting task for the most common scenarios with the following properties.\n",
    "\n",
    "- `target_column_name` - The name of the column to target for predictions. It must always be specified. This parameter is applicable to 'training_data', 'validation_data' and 'test_data'.\n",
    "- `primary_metric` - The metric that AutoML will optimize for model selection.\n",
    "- `training_data` - The data to be used for training. It should contain both training feature columns and a target column. Optionally, this data can be split for segregating a validation or test dataset. \n",
    "You can use a registered MLTable in the workspace using the format '<mltable_name>:<version>' OR you can use a local file or folder as a MLTable. For e.g Input(mltable='my_mltable:1') OR Input(mltable=MLTable(local_path=\"./data\"))\n",
    "The parameter 'training_data' must always be provided.\n",
    "- `compute` - The compute on which the AutoML job will run. In this example we are using serverless compute. You can alternatively use a compute cluster in the workspace. \n",
    "- `name` - The name of the Job/Run. This is an optional property. If not specified, a random name will be generated.\n",
    "- `experiment_name` - The name of the Experiment. An Experiment is like a folder with multiple runs in Azure ML Workspace that should be related to the same logical machine learning experiment.\n",
    "\n",
    "### set_limits() parameters:\n",
    "This is an optional configuration method to configure limits parameters such as timeouts.     \n",
    "    \n",
    "- timeout_minutes - Maximum amount of time in minutes that the whole AutoML job can take before the job terminates. This timeout includes setup, featurization and training runs but does not include the ensembling and model explainability runs at the end of the process since those actions need to happen once all the trials (children jobs) are done. If not specified, the default job's total timeout is 6 days (8,640 minutes). To specify a timeout less than or equal to 1 hour (60 minutes), make sure your dataset's size is not greater than 10,000,000 (rows times column) or an error results.\n",
    "\n",
    "- trial_timeout_minutes - Maximum time in minutes that each trial (child job) can run for before it terminates. If not specified, a value of 1 month or 43200 minutes is used.\n",
    "    \n",
    "- max_trials - The maximum number of trials/runs each with a different combination of algorithm and hyperparameters to try during an AutoML job. If not specified, the default is 1000 trials. If using 'enable_early_termination' the number of trials used can be smaller.\n",
    "    \n",
    "- max_concurrent_trials - Represents the maximum number of trials (children jobs) that would be executed in parallel. It's a good practice to match this number with the number of nodes your cluster.\n",
    "    \n",
    "- enable_early_termination - Whether to enable early termination if the score is not improving in the short term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8836281",
   "metadata": {},
   "source": [
    "## Specialized Forecasting Parameters\n",
    "To define forecasting parameters for your experiment training, you can leverage the .set_forecast_settings() method. \n",
    "The table below details the forecasting parameters we will be passing into our experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
    "|**frequency**|Forecast frequency. This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a03eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general job parameters\n",
    "max_trials = 5\n",
    "exp_name = \"sales-forecasting-experiment\"\n",
    "\n",
    "# Compute cluster name (must exist in your Azure ML workspace)\n",
    "compute_name = \"teslat4-gpu-wus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77eb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AutoML forecasting job with the related factory-function.\n",
    "\n",
    "forecasting_job = automl.forecasting(\n",
    "    # name=\"dpv2-forecasting-job-02\",\n",
    "    experiment_name=exp_name,\n",
    "    compute=compute_name,  # Compute cluster for training\n",
    "    training_data=my_training_data_input,\n",
    "    validation_data=my_validation_data_input,  # Time-based validation (2012 data)\n",
    "    target_column_name=\"demand\",\n",
    "    primary_metric=\"NormalizedRootMeanSquaredError\",\n",
    "    enable_model_explainability=True,\n",
    "    tags={\"retail\": \"forecasting\"},\n",
    ")\n",
    "\n",
    "# Limits are all optional\n",
    "forecasting_job.set_limits(\n",
    "    timeout_minutes=600,\n",
    "    trial_timeout_minutes=20,\n",
    "    max_trials=max_trials,\n",
    "    # max_concurrent_trials = 4,\n",
    "    # max_cores_per_trial: -1,\n",
    "    enable_early_termination=True,\n",
    ")\n",
    "\n",
    "# Specialized properties for Time Series Forecasting training\n",
    "forecasting_job.set_forecast_settings(\n",
    "    time_column_name=\"timeStamp\",\n",
    "    forecast_horizon=52,  # 52 weeks for yearly forecast\n",
    "    frequency=\"W\",        # Weekly frequency (matches our retail data)\n",
    "    target_lags=[1, 2, 4],  # Lag features for 1, 2, and 4 weeks back\n",
    "    target_rolling_window_size=4,\n",
    "    time_series_id_column_names=[\"Store\", \"Dept\"],  # Identify each store-department time series\n",
    "    # short_series_handling_config=ShortSeriesHandlingConfiguration.DROP,\n",
    "    # use_stl=\"season\",\n",
    "    # seasonality=3,\n",
    ")\n",
    "\n",
    "# Training properties are optional\n",
    "forecasting_job.set_training(blocked_training_algorithms=[\"ExtremeRandomTrees\"])\n",
    "\n",
    "# Featurization properties are optional\n",
    "# forecasting_job.set_featurization(# drop_columns=[\"not_needed_column\"], # Optional\n",
    "#                                   # enable_dnn_featurization=True         # Enable if there are text columns\n",
    "#                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16457be8",
   "metadata": {},
   "source": [
    "## Run the Command\n",
    "Using the `MLClient` created earlier, we will now run this Command in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40118475",
   "metadata": {},
   "outputs": [
    {
     "ename": "MlException",
     "evalue": "\n\u001b[37m\n\u001b[30m\n1) One or more fields are invalid\u001b[39m\u001b[39m\n\nDetails: \n\n\u001b[31m(x) Supported input path value are ARM id, AzureML id, remote uri or local path.\nMet <class 'Exception'>:\nKeyBasedAuthenticationNotPermitted\nOperation returned an invalid status 'Key based authentication is not permitted on this storage account.'\nThis SAS token is derived from an account key, but key-based authentication is not permitted for this storage account. To update workspace properties, please see the documentation: https://review.learn.microsoft.com/azure/machine-learning/how-to-disable-local-auth-storage?view=azureml-api-2&branch=pr-en-us-278974&tabs=cli#update-an-existing-workspace\u001b[39m\n\nResolutions: \n1) Double-check that all specified parameters are of the correct types and formats prescribed by the Job schema.\nIf using the CLI, you can also check the full log in debug mode for more details by adding --debug to the end of your command\n\nAdditional Resources: The easiest way to author a yaml specification file is using IntelliSense and auto-completion Azure ML VS code extension provides: \u001b[36mhttps://code.visualstudio.com/docs/datascience/azure-machine-learning.\u001b[39m To set up VS Code, visit \u001b[36mhttps://learn.microsoft.com/azure/machine-learning/how-to-setup-vs-code\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_artifacts/_blob_storage_helper.py:175\u001b[39m, in \u001b[36mBlobStorageClient.check_blob_exists\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     properties = \u001b[43mblob_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_blob_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HttpResponseError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/storage/blob/_blob_client.py:1119\u001b[39m, in \u001b[36mBlobClient.get_blob_properties\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HttpResponseError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1119\u001b[39m     \u001b[43mprocess_storage_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1120\u001b[39m blob_props.name = \u001b[38;5;28mself\u001b[39m.blob_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/storage/blob/_shared/response_handlers.py:195\u001b[39m, in \u001b[36mprocess_storage_error\u001b[39m\u001b[34m(storage_error)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# `from None` prevents us from double printing the exception (suppresses generated layer error context)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraise error from None\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=exec-used # nosec\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\u001b[31mHttpResponseError\u001b[39m: Operation returned an invalid status 'Key based authentication is not permitted on this storage account.'\nErrorCode:KeyBasedAuthenticationNotPermitted",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/operations/_job_operations.py:1358\u001b[39m, in \u001b[36mJobOperations._resolve_job_input\u001b[39m\u001b[34m(self, entry, base_path)\u001b[39m\n\u001b[32m   1357\u001b[39m local_path = Path(base_path \u001b[38;5;129;01mor\u001b[39;00m Path.cwd(), entry.path).resolve()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1358\u001b[39m entry.path = \u001b[43m_upload_and_generate_remote_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_operation_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatastore_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatastore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;66;03m# TODO : Move this part to a common place\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_artifacts/_artifact_utilities.py:417\u001b[39m, in \u001b[36m_upload_and_generate_remote_uri\u001b[39m\u001b[34m(operation_scope, datastore_operation, path, artifact_type, datastore_name, show_progress)\u001b[39m\n\u001b[32m    416\u001b[39m asset_name = \u001b[38;5;28mstr\u001b[39m(uuid.uuid4())\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m artifact_info = \u001b[43m_upload_to_datastore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_scope\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatastore_operation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatastore_operation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatastore_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatastore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43masset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43masset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43martifact_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m path = artifact_info.relative_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_artifacts/_artifact_utilities.py:389\u001b[39m, in \u001b[36m_upload_to_datastore\u001b[39m\u001b[34m(operation_scope, datastore_operation, path, artifact_type, datastore_name, show_progress, asset_name, asset_version, asset_hash, ignore_file, sas_uri, blob_uri)\u001b[39m\n\u001b[32m    388\u001b[39m     asset_hash = get_object_hash(path, ignore_file)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m artifact = \u001b[43mupload_artifact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatastore_operation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatastore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43masset_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43masset_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43masset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43masset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43masset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43masset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43msas_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43msas_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m blob_uri:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_artifacts/_artifact_utilities.py:248\u001b[39m, in \u001b[36mupload_artifact\u001b[39m\u001b[34m(local_path, datastore_operation, operation_scope, datastore_name, asset_hash, show_progress, asset_name, asset_version, ignore_file, sas_uri)\u001b[39m\n\u001b[32m    246\u001b[39m     storage_client = get_storage_client(**datastore_info)\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m artifact_info = \u001b[43mstorage_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43masset_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43masset_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43masset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43masset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m artifact = ArtifactStorageInfo(\n\u001b[32m    258\u001b[39m     name=artifact_info[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    259\u001b[39m     version=artifact_info[\u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m     is_file=Path(local_path).is_file(),\n\u001b[32m    268\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_artifacts/_blob_storage_helper.py:122\u001b[39m, in \u001b[36mBlobStorageClient.upload\u001b[39m\u001b[34m(self, source, name, version, ignore_file, asset_hash, show_progress)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(source):\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[43mupload_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_client\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m=\u001b[49m\u001b[43masset_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_utils/_asset_utils.py:697\u001b[39m, in \u001b[36mupload_directory\u001b[39m\u001b[34m(storage_client, source, dest, msg, show_progress, ignore_file)\u001b[39m\n\u001b[32m    696\u001b[39m     storage_client.indicator_file = upload_paths[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m     \u001b[43mstorage_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_blob_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# Submit paths to workers for upload\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_artifacts/_blob_storage_helper.py:224\u001b[39m, in \u001b[36mBlobStorageClient.check_blob_exists\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationException(\n\u001b[32m    219\u001b[39m         message=msg,\n\u001b[32m    220\u001b[39m         no_personal_data_message=msg,\n\u001b[32m    221\u001b[39m         target=ErrorTarget.ARTIFACT,\n\u001b[32m    222\u001b[39m         error_category=ErrorCategory.USER_ERROR,\n\u001b[32m    223\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_artifacts/_blob_storage_helper.py:181\u001b[39m, in \u001b[36mBlobStorageClient.check_blob_exists\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    180\u001b[39m     exception_with_documentation.__traceback__ = e.exc_traceback\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_with_documentation \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/storage/blob/_blob_client.py:1109\u001b[39m, in \u001b[36mBlobClient.get_blob_properties\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m         kwargs[\u001b[33m'\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m'\u001b[39m] = partial(deserialize_pipeline_response_into_cls, cls_method)\n\u001b[32m-> \u001b[39m\u001b[32m1109\u001b[39m     blob_props = cast(BlobProperties, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeout\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mversion_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m        \u001b[49m\u001b[43msnapshot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msnapshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlease_access_conditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccess_conditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodified_access_conditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmod_conditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcls\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdeserialize_blob_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpk_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpk_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HttpResponseError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/storage/blob/_generated/operations/_blob_operations.py:1945\u001b[39m, in \u001b[36mBlobOperations.get_properties\u001b[39m\u001b[34m(self, snapshot, version_id, timeout, request_id_parameter, lease_access_conditions, cpk_info, modified_access_conditions, **kwargs)\u001b[39m\n\u001b[32m   1944\u001b[39m     error = \u001b[38;5;28mself\u001b[39m._deserialize.failsafe_deserialize(_models.StorageError, pipeline_response)\n\u001b[32m-> \u001b[39m\u001b[32m1945\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response, model=error)\n\u001b[32m   1947\u001b[39m response_headers = {}\n",
      "\u001b[31mException\u001b[39m: KeyBasedAuthenticationNotPermitted\nOperation returned an invalid status 'Key based authentication is not permitted on this storage account.'\nThis SAS token is derived from an account key, but key-based authentication is not permitted for this storage account. To update workspace properties, please see the documentation: https://review.learn.microsoft.com/azure/machine-learning/how-to-disable-local-auth-storage?view=azureml-api-2&branch=pr-en-us-278974&tabs=cli#update-an-existing-workspace",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValidationException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/operations/_job_operations.py:680\u001b[39m, in \u001b[36mJobOperations.create_or_update\u001b[39m\u001b[34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[39m\n\u001b[32m    679\u001b[39m     \u001b[38;5;66;03m# Create all dependent resources\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve_arm_id_or_upload_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationException, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/operations/_job_operations.py:1146\u001b[39m, in \u001b[36mJobOperations._resolve_arm_id_or_upload_dependencies\u001b[39m\u001b[34m(self, job)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, AutoMLJob):\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve_automl_job_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, FineTuningJob):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/operations/_job_operations.py:1175\u001b[39m, in \u001b[36mJobOperations._resolve_automl_job_inputs\u001b[39m\u001b[34m(self, job)\u001b[39m\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, AutoMLJob):\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve_job_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_base_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m job.validation_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/operations/_job_operations.py:1371\u001b[39m, in \u001b[36mJobOperations._resolve_job_input\u001b[39m\u001b[34m(self, entry, base_path)\u001b[39m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationException(\n\u001b[32m   1372\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSupported input path value are ARM id, AzureML id, remote uri or local path.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1374\u001b[39m         target=ErrorTarget.JOB,\n\u001b[32m   1375\u001b[39m         no_personal_data_message=(\n\u001b[32m   1376\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSupported input path value are ARM id, AzureML id, \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mremote uri or local path.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1377\u001b[39m         ),\n\u001b[32m   1378\u001b[39m         error=e,\n\u001b[32m   1379\u001b[39m         error_category=ErrorCategory.USER_ERROR,\n\u001b[32m   1380\u001b[39m         error_type=ValidationErrorType.INVALID_VALUE,\n\u001b[32m   1381\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mValidationException\u001b[39m: Supported input path value are ARM id, AzureML id, remote uri or local path.\nMet <class 'Exception'>:\nKeyBasedAuthenticationNotPermitted\nOperation returned an invalid status 'Key based authentication is not permitted on this storage account.'\nThis SAS token is derived from an account key, but key-based authentication is not permitted for this storage account. To update workspace properties, please see the documentation: https://review.learn.microsoft.com/azure/machine-learning/how-to-disable-local-auth-storage?view=azureml-api-2&branch=pr-en-us-278974&tabs=cli#update-an-existing-workspace",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mMlException\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Submit the AutoML job\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m returned_job = \u001b[43mml_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforecasting_job\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m  \u001b[38;5;66;03m# submit the job to the backend\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated job: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturned_job\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_telemetry/activity.py:371\u001b[39m, in \u001b[36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m dimensions = {**parameter_dimensions, **(custom_dimensions \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, dimensions) \u001b[38;5;28;01mas\u001b[39;00m activityLogger:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     return_value = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter_dimensions:\n\u001b[32m    373\u001b[39m         \u001b[38;5;66;03m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[32m    374\u001b[39m         activityLogger.activity_info.update(_collect_from_return_value(return_value))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/operations/_job_operations.py:682\u001b[39m, in \u001b[36mJobOperations.create_or_update\u001b[39m\u001b[34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m     \u001b[38;5;28mself\u001b[39m._resolve_arm_id_or_upload_dependencies(job)\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationException, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     \u001b[43mlog_and_raise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m git_props = get_git_properties()\n\u001b[32m    685\u001b[39m \u001b[38;5;66;03m# Do not add git props if they already exist in job properties.\u001b[39;00m\n\u001b[32m    686\u001b[39m \u001b[38;5;66;03m# This is for update specifically-- if the user switches branches and tries to update\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# their job, the request will fail since the git props will be repopulated.\u001b[39;00m\n\u001b[32m    688\u001b[39m \u001b[38;5;66;03m# MFE does not allow existing properties to be updated, only for new props to be added\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/foundry-mcp-reservation-system/.venv/lib/python3.13/site-packages/azure/ai/ml/_exception_helper.py:337\u001b[39m, in \u001b[36mlog_and_raise_error\u001b[39m\u001b[34m(error, debug, yaml_operation)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m MlException(message=formatted_error, no_personal_data_message=formatted_error)\n",
      "\u001b[31mMlException\u001b[39m: \n\u001b[37m\n\u001b[30m\n1) One or more fields are invalid\u001b[39m\u001b[39m\n\nDetails: \n\n\u001b[31m(x) Supported input path value are ARM id, AzureML id, remote uri or local path.\nMet <class 'Exception'>:\nKeyBasedAuthenticationNotPermitted\nOperation returned an invalid status 'Key based authentication is not permitted on this storage account.'\nThis SAS token is derived from an account key, but key-based authentication is not permitted for this storage account. To update workspace properties, please see the documentation: https://review.learn.microsoft.com/azure/machine-learning/how-to-disable-local-auth-storage?view=azureml-api-2&branch=pr-en-us-278974&tabs=cli#update-an-existing-workspace\u001b[39m\n\nResolutions: \n1) Double-check that all specified parameters are of the correct types and formats prescribed by the Job schema.\nIf using the CLI, you can also check the full log in debug mode for more details by adding --debug to the end of your command\n\nAdditional Resources: The easiest way to author a yaml specification file is using IntelliSense and auto-completion Azure ML VS code extension provides: \u001b[36mhttps://code.visualstudio.com/docs/datascience/azure-machine-learning.\u001b[39m To set up VS Code, visit \u001b[36mhttps://learn.microsoft.com/azure/machine-learning/how-to-setup-vs-code\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Submit the AutoML job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    forecasting_job\n",
    ")  # submit the job to the backend\n",
    "\n",
    "print(f\"Created job: {returned_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8129921",
   "metadata": {},
   "source": [
    "# 4. Get Predictions from the Trained Model\n",
    "\n",
    "After the AutoML job completes, we can download the best model and generate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b636706",
   "metadata": {},
   "source": [
    "## 4.1 Download the Best Model\n",
    "\n",
    "Download the best performing model from the completed AutoML job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b90dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to complete (if not already)\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "# Get the completed job details\n",
    "completed_job = ml_client.jobs.get(returned_job.name)\n",
    "print(f\"Job status: {completed_job.status}\")\n",
    "\n",
    "# Download the best model artifacts\n",
    "model_download_path = \"./outputs/best_model\"\n",
    "os.makedirs(model_download_path, exist_ok=True)\n",
    "\n",
    "ml_client.jobs.download(\n",
    "    name=returned_job.name,\n",
    "    download_path=\"./outputs\",\n",
    "    output_name=\"best_model\"\n",
    ")\n",
    "\n",
    "print(f\"Best model downloaded to: {model_download_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08e102",
   "metadata": {},
   "source": [
    "## 4.2 Register the Model (Optional)\n",
    "\n",
    "Register the best model in Azure ML for versioning and deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d1291",
   "metadata": {},
   "source": [
    "## 4.3 Load Model and Generate Predictions\n",
    "\n",
    "Load the downloaded model and generate forecasts on the validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b922eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Load the downloaded model\n",
    "model_path = \"./outputs/best_model\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "print(f\"Model loaded successfully from: {model_path}\")\n",
    "print(f\"Model flavor: {loaded_model.metadata.flavors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data for prediction (remove target column)\n",
    "prediction_input = validation_df.drop(columns=['demand']).copy()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = loaded_model.predict(prediction_input)\n",
    "\n",
    "# Add predictions to validation dataframe\n",
    "validation_df['predicted_demand'] = predictions\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions\")\n",
    "display(validation_df[['Store', 'Dept', 'timeStamp', 'demand', 'predicted_demand']].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612aef4",
   "metadata": {},
   "source": [
    "## 4.4 Evaluate Predictions\n",
    "\n",
    "Calculate error metrics to assess model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "actual_tensor = torch.tensor(validation_df['demand'].values, dtype=torch.float32)\n",
    "predicted_tensor = torch.tensor(validation_df['predicted_demand'].values, dtype=torch.float32)\n",
    "\n",
    "# Calculate evaluation metrics using PyTorch\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = torch.mean(torch.abs(actual_tensor - predicted_tensor)).item()\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "mse = torch.mean((actual_tensor - predicted_tensor) ** 2)\n",
    "rmse = torch.sqrt(mse).item()\n",
    "\n",
    "# RÂ² Score\n",
    "ss_res = torch.sum((actual_tensor - predicted_tensor) ** 2)\n",
    "ss_tot = torch.sum((actual_tensor - torch.mean(actual_tensor)) ** 2)\n",
    "r2 = (1 - ss_res / ss_tot).item()\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE) - handle zeros\n",
    "non_zero_mask = actual_tensor != 0\n",
    "mape = torch.mean(torch.abs((actual_tensor[non_zero_mask] - predicted_tensor[non_zero_mask]) / actual_tensor[non_zero_mask])).item() * 100\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION METRICS (PyTorch)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Absolute Error (MAE):        ${mae:,.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE):   ${rmse:,.2f}\")\n",
    "print(f\"RÂ² Score:                          {r2:.4f}\")\n",
    "print(f\"Mean Absolute % Error (MAPE):      {mape:.2f}%\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97837621",
   "metadata": {},
   "source": [
    "## 4.5 Save Predictions\n",
    "\n",
    "Export the predictions to a CSV file for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "output_file = \"./outputs/predictions.csv\"\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "# Select relevant columns for output\n",
    "prediction_output = validation_df[['Store', 'Dept', 'timeStamp', 'demand', 'predicted_demand']].copy()\n",
    "prediction_output['error'] = prediction_output['demand'] - prediction_output['predicted_demand']\n",
    "prediction_output['absolute_error'] = abs(prediction_output['error'])\n",
    "\n",
    "prediction_output.to_csv(output_file, index=False)\n",
    "print(f\"Predictions saved to: {output_file}\")\n",
    "print(f\"Total predictions: {len(prediction_output)}\")\n",
    "\n",
    "# Show summary by store\n",
    "print(\"\\n--- Prediction Summary by Store (Top 10) ---\")\n",
    "store_summary = prediction_output.groupby('Store').agg({\n",
    "    'demand': 'sum',\n",
    "    'predicted_demand': 'sum',\n",
    "    'absolute_error': 'mean'\n",
    "}).round(2)\n",
    "store_summary.columns = ['Actual Sales', 'Predicted Sales', 'Avg Absolute Error']\n",
    "display(store_summary.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the best model in Azure ML Model Registry\n",
    "model = Model(\n",
    "    path=f\"azureml://jobs/{returned_job.name}/outputs/best_model\",\n",
    "    name=\"retail-sales-forecasting-model\",\n",
    "    description=\"AutoML time-series forecasting model for retail weekly sales\",\n",
    "    type=\"mlflow_model\"\n",
    ")\n",
    "\n",
    "registered_model = ml_client.models.create_or_update(model)\n",
    "print(f\"Registered model: {registered_model.name}, version: {registered_model.version}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
