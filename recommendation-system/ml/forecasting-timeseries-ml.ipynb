{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6d4eda",
   "metadata": {},
   "source": [
    "# AutoML: Train \"the best\" Time-Series Forecasting model for Retail Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f23d42",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c55927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress OpenTelemetry warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Overriding of current\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Attempting to instrument\")\n",
    "\n",
    "# Suppress Azure SDK telemetry logging\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "credential = AzureCliCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    subscription_id = \"57123c17-af1a-4ec2-9494-a214fb148bf4\"\n",
    "    resource_group = \"admin-rg\"\n",
    "    workspace = \"ml-demo-wksp-wus-01\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "except Exception as ex:\n",
    "    print(\"Ex:\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection\n",
    "ws = ml_client.workspaces.get(ml_client.workspace_name)\n",
    "print(f\"Connected to: {ws.name} ({ws.location})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ebeb9",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "Using [Retail data analytics](https://www.kaggle.com/datasets/manjeetsingh/retaildataset) - weekly sales by store and department."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e5c30",
   "metadata": {},
   "source": [
    "## 2.1 Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a98900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "stores_df = pd.read_csv('../dataset/stores data-set.csv')\n",
    "features_df = pd.read_csv('../dataset/Features data set.csv')\n",
    "sales_df = pd.read_csv('../dataset/sales data-set.csv')\n",
    "\n",
    "# Quick exploration\n",
    "print(f\"Stores: {stores_df.shape}\")\n",
    "print(f\"Features: {features_df.shape}\")\n",
    "print(f\"Sales: {sales_df.shape}\")\n",
    "\n",
    "print(\"\\n--- Stores Data ---\")\n",
    "display(stores_df.head())\n",
    "\n",
    "print(\"\\n--- Features Data ---\")\n",
    "display(features_df.head())\n",
    "\n",
    "print(\"\\n--- Sales Data ---\")\n",
    "display(sales_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffc88b",
   "metadata": {},
   "source": [
    "## 2.2 Merge Datasets\n",
    "Merge sales with stores (on Store) and then with features (on Store and Date).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales with stores (on Store)\n",
    "merged_df = sales_df.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "# Merge with features (on Store and Date)\n",
    "merged_df = merged_df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
    "\n",
    "# Drop duplicate IsHoliday column from features\n",
    "merged_df = merged_df.drop(columns=['IsHoliday_feat'])\n",
    "\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "print(f\"\\nColumns: {merged_df.columns.tolist()}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e3f01",
   "metadata": {},
   "source": [
    "## 2.3 Feature Engineering\n",
    "Create new features from date, handle missing MarkDown values, and encode categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to datetime (format is dd/mm/yyyy)\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "\n",
    "# Extract date features\n",
    "merged_df['Year'] = merged_df['Date'].dt.year\n",
    "merged_df['Month'] = merged_df['Date'].dt.month\n",
    "merged_df['Week'] = merged_df['Date'].dt.isocalendar().week\n",
    "merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "# Handle missing MarkDown values (only available after Nov 2011)\n",
    "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "merged_df[markdown_cols] = merged_df[markdown_cols].fillna(0)\n",
    "\n",
    "# Encode categorical: Store Type (A, B, C)\n",
    "if 'Type' in merged_df.columns:\n",
    "    merged_df = pd.get_dummies(merged_df, columns=['Type'], prefix='StoreType')\n",
    "\n",
    "print(f\"Feature engineered dataset: {merged_df.shape}\")\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0205d",
   "metadata": {},
   "source": [
    "## 2.4 Time-Based Train/Validation Split\n",
    "\n",
    "Split data chronologically: train on data before 2012, validate on 2012 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ab93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "dupes = merged_df.groupby(['Store', 'Dept', 'Date']).size()\n",
    "print(f\"Duplicate combinations: {(dupes > 1).sum()}\")\n",
    "\n",
    "# Aggregate duplicates: sum Weekly_Sales, take first for other columns\n",
    "agg_funcs = {col: 'first' for col in merged_df.columns if col not in ['Store', 'Dept', 'Date']}\n",
    "agg_funcs['Weekly_Sales'] = 'sum'\n",
    "merged_df = merged_df.groupby(['Store', 'Dept', 'Date'], as_index=False).agg(agg_funcs)\n",
    "print(f\"After deduplication: {merged_df.shape}\")\n",
    "\n",
    "# Sort by date\n",
    "merged_df = merged_df.sort_values(['Store', 'Dept', 'Date'])\n",
    "\n",
    "# Time-based split: train on data before 2012, validate on 2012\n",
    "train_df = merged_df[merged_df['Year'] < 2012].copy()\n",
    "validation_df = merged_df[merged_df['Year'] >= 2012].copy()\n",
    "\n",
    "print(f\"\\nTraining set: {train_df.shape}\")\n",
    "print(f\"Validation set: {validation_df.shape}\")\n",
    "print(f\"Train date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
    "print(f\"Validation date range: {validation_df['Date'].min()} to {validation_df['Date'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00bca9",
   "metadata": {},
   "source": [
    "## 2.5 Prepare Data for Azure ML AutoML\n",
    "\n",
    "Rename columns to match AutoML expectations and save as MLTable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Rename columns for AutoML compatibility\n",
    "train_df = train_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "validation_df = validation_df.rename(columns={'Weekly_Sales': 'demand', 'Date': 'timeStamp'})\n",
    "\n",
    "# Create single time series ID column (before converting dates to strings)\n",
    "train_df['ts_id'] = train_df['Store'].astype(str) + '_' + train_df['Dept'].astype(str)\n",
    "validation_df['ts_id'] = validation_df['Store'].astype(str) + '_' + validation_df['Dept'].astype(str)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Filter to common time series IDs (both train and validation must have same IDs)\n",
    "# ============================================================================\n",
    "train_ts_ids = set(train_df['ts_id'].unique())\n",
    "val_ts_ids = set(validation_df['ts_id'].unique())\n",
    "\n",
    "val_only_ids = val_ts_ids - train_ts_ids\n",
    "train_only_ids = train_ts_ids - val_ts_ids\n",
    "common_ids = train_ts_ids & val_ts_ids\n",
    "\n",
    "print(f\"=== Step 1: Filter to common time series ===\")\n",
    "print(f\"Time series in training only: {len(train_only_ids)}\")\n",
    "print(f\"Time series in validation only: {len(val_only_ids)}\")\n",
    "print(f\"Common time series: {len(common_ids)}\")\n",
    "\n",
    "# Keep only common IDs\n",
    "train_df = train_df[train_df['ts_id'].isin(common_ids)]\n",
    "validation_df = validation_df[validation_df['ts_id'].isin(common_ids)]\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Check contiguity - validation must start right after training ends\n",
    "# For weekly data, the gap should be exactly 7 days\n",
    "# ============================================================================\n",
    "print(f\"\\n=== Step 2: Check contiguity (no gaps between train and validation) ===\")\n",
    "\n",
    "# Get max date per ts_id in training\n",
    "train_max_dates = train_df.groupby('ts_id')['timeStamp'].max().reset_index()\n",
    "train_max_dates.columns = ['ts_id', 'train_max_date']\n",
    "\n",
    "# Get min date per ts_id in validation\n",
    "val_min_dates = validation_df.groupby('ts_id')['timeStamp'].min().reset_index()\n",
    "val_min_dates.columns = ['ts_id', 'val_min_date']\n",
    "\n",
    "# Merge to compare\n",
    "contiguity_check = train_max_dates.merge(val_min_dates, on='ts_id')\n",
    "contiguity_check['train_max_date'] = pd.to_datetime(contiguity_check['train_max_date'])\n",
    "contiguity_check['val_min_date'] = pd.to_datetime(contiguity_check['val_min_date'])\n",
    "contiguity_check['gap_days'] = (contiguity_check['val_min_date'] - contiguity_check['train_max_date']).dt.days\n",
    "\n",
    "# For weekly data, gap should be 7 days (next week)\n",
    "# Allow some flexibility: 6-8 days is acceptable\n",
    "contiguity_check['is_contiguous'] = contiguity_check['gap_days'].between(6, 8)\n",
    "\n",
    "non_contiguous = contiguity_check[~contiguity_check['is_contiguous']]\n",
    "contiguous_ids = set(contiguity_check[contiguity_check['is_contiguous']]['ts_id'])\n",
    "\n",
    "print(f\"Contiguous time series: {len(contiguous_ids)}\")\n",
    "print(f\"Non-contiguous time series (will be removed): {len(non_contiguous)}\")\n",
    "\n",
    "if len(non_contiguous) > 0:\n",
    "    print(f\"\\nSample non-contiguous series:\")\n",
    "    sample = non_contiguous.head(10)\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"  {row['ts_id']}: train ends {row['train_max_date'].date()}, val starts {row['val_min_date'].date()} (gap: {row['gap_days']} days)\")\n",
    "\n",
    "# Filter to only contiguous time series\n",
    "train_df = train_df[train_df['ts_id'].isin(contiguous_ids)]\n",
    "validation_df = validation_df[validation_df['ts_id'].isin(contiguous_ids)]\n",
    "\n",
    "print(f\"\\n=== Final Dataset ===\")\n",
    "print(f\"Training time series: {train_df['ts_id'].nunique()}\")\n",
    "print(f\"Validation time series: {validation_df['ts_id'].nunique()}\")\n",
    "print(f\"Training rows: {len(train_df)}\")\n",
    "print(f\"Validation rows: {len(validation_df)}\")\n",
    "\n",
    "# Convert timestamp to consistent date string format (no time component)\n",
    "train_df['timeStamp'] = pd.to_datetime(train_df['timeStamp']).dt.strftime('%Y-%m-%d')\n",
    "validation_df['timeStamp'] = pd.to_datetime(validation_df['timeStamp']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Verify no duplicates\n",
    "train_dupes = train_df.duplicated(subset=['ts_id', 'timeStamp']).sum()\n",
    "val_dupes = validation_df.duplicated(subset=['ts_id', 'timeStamp']).sum()\n",
    "print(f\"Train duplicates: {train_dupes}, Validation duplicates: {val_dupes}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('./data/training-mltable-folder', exist_ok=True)\n",
    "os.makedirs('./data/validation-mltable-folder', exist_ok=True)\n",
    "\n",
    "# Save as CSV (MLTable will reference these)\n",
    "train_df.to_csv('./data/training-mltable-folder/train.csv', index=False)\n",
    "validation_df.to_csv('./data/validation-mltable-folder/validation.csv', index=False)\n",
    "\n",
    "print(f\"\\nTraining data saved: {len(train_df)} rows\")\n",
    "print(f\"Validation data saved: {len(validation_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mltable_train = \"\"\"paths:\n",
    "  - file: ./train.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "mltable_val = \"\"\"paths:\n",
    "  - file: ./validation.csv\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      header: all_files_same_headers\n",
    "\"\"\"\n",
    "\n",
    "with open('./data/training-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_train)\n",
    "    \n",
    "with open('./data/validation-mltable-folder/MLTable', 'w') as f:\n",
    "    f.write(mltable_val)\n",
    "\n",
    "print(\"MLTable files created:\")\n",
    "print(\"  - ./data/training-mltable-folder/MLTable\")\n",
    "print(\"  - ./data/validation-mltable-folder/MLTable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861505be",
   "metadata": {},
   "source": [
    "## 2.6 Upload Data to Azure Blob Storage\n",
    "\n",
    "Due to Azure Policy restrictions (SAS tokens disabled), data must be uploaded using Azure CLI with OAuth authentication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80df692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Azure Storage configuration\n",
    "STORAGE_ACCOUNT = \"mldemowkspwus02609576373\"\n",
    "CONTAINER = \"azureml-blobstore-cff56e3a-d016-4526-aa58-71c460675066\"\n",
    "\n",
    "def upload_to_blob(source_folder, destination_path):\n",
    "    \"\"\"Upload local folder to Azure Blob Storage using OAuth authentication.\"\"\"\n",
    "    # First, delete existing data to ensure fresh upload\n",
    "    delete_cmd = [\n",
    "        \"az\", \"storage\", \"blob\", \"delete-batch\",\n",
    "        \"--account-name\", STORAGE_ACCOUNT,\n",
    "        \"--source\", CONTAINER,\n",
    "        \"--pattern\", f\"{destination_path}/*\",\n",
    "        \"--auth-mode\", \"login\"\n",
    "    ]\n",
    "    print(f\"Cleaning {destination_path}...\")\n",
    "    subprocess.run(delete_cmd, capture_output=True, text=True)\n",
    "    \n",
    "    # Upload new data\n",
    "    upload_cmd = [\n",
    "        \"az\", \"storage\", \"blob\", \"upload-batch\",\n",
    "        \"--account-name\", STORAGE_ACCOUNT,\n",
    "        \"--destination\", CONTAINER,\n",
    "        \"--destination-path\", destination_path,\n",
    "        \"--source\", source_folder,\n",
    "        \"--auth-mode\", \"login\",\n",
    "        \"--overwrite\"\n",
    "    ]\n",
    "    print(f\"Uploading {source_folder} to {destination_path}...\")\n",
    "    result = subprocess.run(upload_cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úì Successfully uploaded to {destination_path}\")\n",
    "        print(f\"  Output: {result.stdout[:200] if result.stdout else 'OK'}\")\n",
    "    else:\n",
    "        print(f\"‚úó Upload failed: {result.stderr}\")\n",
    "    return result.returncode == 0\n",
    "\n",
    "# Upload training data to NEW path\n",
    "upload_to_blob(\"./data/training-mltable-folder\", \"retail-train-v2\")\n",
    "\n",
    "# Upload validation data to NEW path\n",
    "upload_to_blob(\"./data/validation-mltable-folder\", \"retail-val-v2\")\n",
    "\n",
    "print(\"\\nData upload complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc993c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, \n",
    "    path=\"azureml://datastores/workspaceblobstore_identity/paths/retail-train-v2\"\n",
    ")\n",
    "\n",
    "my_validation_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, \n",
    "    path=\"azureml://datastores/workspaceblobstore_identity/paths/retail-val-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929c4a9",
   "metadata": {},
   "source": [
    "# 3. Configure and Run AutoML Forecasting Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8836281",
   "metadata": {},
   "source": [
    "## 3.1 Job Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AutoML forecasting job with the related factory-function.\n",
    "forecasting_job = automl.forecasting(\n",
    "    experiment_name=\"sales-forecasting-v2\",\n",
    "    compute=\"teslat4-gpu-wus\",  \n",
    "    training_data=my_training_data_input,\n",
    "    validation_data=my_validation_data_input, \n",
    "    target_column_name=\"demand\",\n",
    "    primary_metric=\"NormalizedRootMeanSquaredError\",\n",
    "    enable_model_explainability=True,\n",
    "    tags={\"retail\": \"forecasting\"},\n",
    ")\n",
    "\n",
    "# Limits are all optional\n",
    "forecasting_job.set_limits(\n",
    "    timeout_minutes=600,\n",
    "    trial_timeout_minutes=20,\n",
    "    max_trials=5,\n",
    "    enable_early_termination=True,\n",
    ")\n",
    "\n",
    "# Specialized properties for Time Series Forecasting training\n",
    "forecasting_job.set_forecast_settings(\n",
    "    time_column_name=\"timeStamp\",\n",
    "    forecast_horizon=12,  # 12 weeks forecast\n",
    "    frequency=\"W-FRI\",  # pandas offset: W-FRI=weekly anchored on Friday (matches our data)\n",
    "    time_series_id_column_names=[\"ts_id\"],\n",
    "    short_series_handling_config=\"auto\",  # Auto-handle short/irregular series\n",
    "    target_lags=\"auto\",\n",
    ")\n",
    "\n",
    "# forecasting_job.set_training(blocked_training_algorithms=[\"ExtremeRandomTrees\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16457be8",
   "metadata": {},
   "source": [
    "## 3.2 Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40118475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the AutoML job\n",
    "returned_job = ml_client.jobs.create_or_update(forecasting_job)\n",
    "print(f\"Created job: {returned_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8129921",
   "metadata": {},
   "source": [
    "# 4. Register the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTE: The model requires Azure ML runtime which doesn't work on Apple Silicon.\n",
    "# Instead, we'll register the model and run batch inference in Azure ML.\n",
    "# ============================================================================\n",
    "\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "# Register the best model in Azure ML Model Registry\n",
    "print(\"Registering model in Azure ML...\")\n",
    "\n",
    "model = Model(\n",
    "    path=f\"azureml://jobs/{returned_job.name}/outputs/best_model\",\n",
    "    name=\"retail-sales-forecasting-model\",\n",
    "    description=\"AutoML time-series forecasting model for retail weekly sales\",\n",
    "    type=\"mlflow_model\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    registered_model = ml_client.models.create_or_update(model)\n",
    "    print(f\"‚úì Registered model: {registered_model.name}, version: {registered_model.version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model may already be registered: {e}\")\n",
    "    # Get existing model\n",
    "    registered_model = ml_client.models.get(name=\"retail-sales-forecasting-model\", version=\"latest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e1d0f",
   "metadata": {},
   "source": [
    "# 5. Deploy and Test the Model\n",
    "\n",
    "Deploy the model as a Managed Online Endpoint to test predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "905eae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint: retail-forecast-12022118\n",
      "This may take 5-10 minutes...\n",
      "‚úì Endpoint created: retail-forecast-12022118\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Create a Managed Online Endpoint\n",
    "# ============================================================================\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Create a unique endpoint name\n",
    "endpoint_name = f\"retail-forecast-{datetime.datetime.now().strftime('%m%d%H%M')}\"\n",
    "\n",
    "# Define the endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"Retail sales forecasting endpoint\",\n",
    "    auth_mode=\"key\"  # or \"aml_token\" for Azure AD auth\n",
    ")\n",
    "\n",
    "# Create the endpoint (this takes a few minutes)\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "print(\"This may take 5-10 minutes...\")\n",
    "\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "print(f\"‚úì Endpoint created: {endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d03a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint retail-forecast-12022118 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: retail-sales-forecasting-model, version: 1\n",
      "Deploying model to endpoint...\n",
      "This may take 10-15 minutes...\n",
      "...............................................................................................‚úì Model deployed to: retail-forecast-12022118\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Deploy the Model to the Endpoint\n",
    "# ============================================================================\n",
    "\n",
    "# Get the registered model\n",
    "model = ml_client.models.get(name=\"retail-sales-forecasting-model\", version=\"1\")\n",
    "print(f\"Using model: {model.name}, version: {model.version}\")\n",
    "\n",
    "# Create the deployment with extended timeout (model takes ~90s for first inference)\n",
    "from azure.ai.ml.entities import OnlineRequestSettings\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=\"default\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",  # 4 cores, 14 GB RAM\n",
    "    instance_count=1,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        request_timeout_ms=180000,  # 3 minutes timeout\n",
    "        max_concurrent_requests_per_instance=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Deploy (this takes 5-15 minutes)\n",
    "print(f\"Deploying model to endpoint...\")\n",
    "print(\"This may take 10-15 minutes...\")\n",
    "\n",
    "ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
    "\n",
    "# Set the deployment to receive 100% of traffic\n",
    "endpoint.traffic = {\"default\": 100}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "\n",
    "print(f\"‚úì Model deployed to: {endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7422dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INPUT DATA FOR FORECASTING ===\n",
      "\n",
      "History (5 rows with actual demand):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>demand</th>\n",
       "      <th>ts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>16567.69</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>16894.40</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-20</td>\n",
       "      <td>18365.10</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-27</td>\n",
       "      <td>18378.16</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-02-03</td>\n",
       "      <td>23510.49</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timeStamp    demand ts_id\n",
       "0  2012-01-06  16567.69   1_1\n",
       "1  2012-01-13  16894.40   1_1\n",
       "2  2012-01-20  18365.10   1_1\n",
       "3  2012-01-27  18378.16   1_1\n",
       "4  2012-02-03  23510.49   1_1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Future (2 rows with demand=NaN - to be predicted):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>demand</th>\n",
       "      <th>ts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-02-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timeStamp  demand ts_id\n",
       "5  2012-02-10     NaN   1_1\n",
       "6  2012-02-17     NaN   1_1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual values we want to predict: [36988.49, 54060.1]\n",
      "\n",
      "‚úì Request saved with 5 history rows + 2 forecast rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Test the Endpoint with Sample Data\n",
    "# ============================================================================\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load validation data\n",
    "test_df = pd.read_csv('./data/validation-mltable-folder/validation.csv')\n",
    "\n",
    "# Get data for one time series\n",
    "ts_data = test_df[test_df['ts_id'] == '1_1'].copy().reset_index(drop=True)\n",
    "\n",
    "# ============================================================================\n",
    "# KEY INSIGHT: AutoML forecasting needs:\n",
    "#   - Historical rows WITH actual 'demand' values (for context/lags)\n",
    "#   - Future rows WITH demand = NaN (rows we want to predict)\n",
    "# ============================================================================\n",
    "\n",
    "# Use first 5 rows as HISTORY (with actual values)\n",
    "# Use next 2 rows as FUTURE (set demand to NaN - these will be predicted)\n",
    "history = ts_data.head(5).copy()\n",
    "future = ts_data.iloc[5:7].copy()\n",
    "\n",
    "# Store actual values for comparison\n",
    "actual_demand = future['demand'].tolist()\n",
    "\n",
    "# Set future demand to NaN - this tells the model to forecast these\n",
    "future['demand'] = np.nan\n",
    "\n",
    "# Combine history + future\n",
    "forecast_input = pd.concat([history, future], ignore_index=True)\n",
    "\n",
    "print(\"=== INPUT DATA FOR FORECASTING ===\")\n",
    "print(f\"\\nHistory (5 rows with actual demand):\")\n",
    "display(history[['timeStamp', 'demand', 'ts_id']].head())\n",
    "\n",
    "print(f\"\\nFuture (2 rows with demand=NaN - to be predicted):\")\n",
    "display(future[['timeStamp', 'demand', 'ts_id']])\n",
    "\n",
    "print(f\"\\nActual values we want to predict: {actual_demand}\")\n",
    "\n",
    "# AutoML expects pandas-split format\n",
    "# Convert NaN to None for JSON serialization (NaN is not valid JSON)\n",
    "def nan_to_none(val):\n",
    "    \"\"\"Convert numpy NaN to Python None for JSON compatibility.\"\"\"\n",
    "    if isinstance(val, float) and np.isnan(val):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "# Convert data, replacing NaN with None\n",
    "data_cleaned = [[nan_to_none(v) for v in row] for row in forecast_input.values.tolist()]\n",
    "\n",
    "input_data = {\n",
    "    \"input_data\": {\n",
    "        \"columns\": forecast_input.columns.tolist(),\n",
    "        \"index\": list(range(len(forecast_input))),\n",
    "        \"data\": data_cleaned\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file (None becomes null in JSON, which AutoML understands as missing)\n",
    "with open(\"./sample_request.json\", \"w\") as f:\n",
    "    json.dump(input_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Request saved with {len(history)} history rows + {len(future)} forecast rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed95aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling endpoint: retail-forecast-12022118\n",
      "==================================================\n",
      "Scoring URI: https://retail-forecast-12022118.westus.inference.ml.azure.com/score\n",
      "Using 60-second timeout...\n",
      "\n",
      "‚ùå Error: 408 Client Error: Request Timeout for url: https://retail-forecast-12022118.westus.inference.ml.azure.com/score\n",
      "\n",
      "üìã Getting logs...\n",
      " [03/Dec/2025:02:40:03 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\n",
      "2025-12-03 02:40:13,664 I [71] gunicorn.access - 127.0.0.1 - - [03/Dec/2025:02:40:13 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\n",
      "2025-12-03 02:40:13,665 I [71] gunicorn.access - 127.0.0.1 - - [03/Dec/2025:02:40:13 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\n",
      "/azureml-envs/azureml-automl/lib/python3.9/site-packages/azureml/training/tabular/models/forecasting_pipeline_wrapper_base.py:1596: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for grain, df_one in X_copy.groupby(self.grain_column_list):\n",
      "/azureml-envs/azureml-automl/lib/python3.9/site-packages/azureml/training/tabular/models/forecasting_pipeline_wrapper_base.py:885: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for tsid, df_one in Xy_pred.groupby(self.grain_column_names):\n",
      "/azureml-envs/azureml-automl/lib/python3.9/site-packages/azureml/training/tabular/timeseries/_frequency_fixer.py:577: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for grain, df in X.groupby(groupers, as_index=False, group_keys=False):\n",
      "/azureml-envs/azureml-automl/lib/python3.9/site-packages/azureml/training/tabular/timeseries/_frequency_fixer.py:577: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for grain, df in X.groupby(groupers, as_index=False, group_keys=False):\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Invoke the Endpoint to Get Predictions\n",
    "# ============================================================================\n",
    "import requests\n",
    "\n",
    "print(f\"Calling endpoint: {endpoint_name}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# The model takes ~22 seconds. Use requests with longer timeout.\n",
    "# Get endpoint details\n",
    "endpoint_info = ml_client.online_endpoints.get(endpoint_name)\n",
    "scoring_uri = endpoint_info.scoring_uri\n",
    "api_key = ml_client.online_endpoints.get_keys(endpoint_name).primary_key\n",
    "\n",
    "print(f\"Scoring URI: {scoring_uri}\")\n",
    "print(\"Using 60-second timeout...\")\n",
    "\n",
    "try:\n",
    "    # Read the request file\n",
    "    with open(\"./sample_request.json\", \"r\") as f:\n",
    "        request_data = json.load(f)\n",
    "    \n",
    "    # Call with extended timeout\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    resp = requests.post(scoring_uri, json=request_data, headers=headers, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    response = resp.text\n",
    "\n",
    "    print(f\"Raw response: {response[:500]}\")\n",
    "    \n",
    "    # Parse the response\n",
    "    predictions = json.loads(response)\n",
    "    print(\"\\nüìä PREDICTIONS:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Handle different response formats\n",
    "    if isinstance(predictions, list):\n",
    "        pred_values = predictions\n",
    "    elif isinstance(predictions, dict):\n",
    "        pred_values = predictions.get('predictions', predictions.get('forecast', list(predictions.values())[0]))\n",
    "    else:\n",
    "        pred_values = [predictions]\n",
    "    \n",
    "    print(f\"Predicted values: {pred_values}\")\n",
    "    print(f\"Actual values:    {actual_demand}\")\n",
    "    \n",
    "    # Calculate error\n",
    "    if len(pred_values) == len(actual_demand):\n",
    "        for i, (pred, actual) in enumerate(zip(pred_values, actual_demand)):\n",
    "            error = actual - pred\n",
    "            pct_error = (error / actual) * 100 if actual != 0 else 0\n",
    "            print(f\"  Row {i+1}: Predicted={pred:.2f}, Actual={actual:.2f}, Error={error:.2f} ({pct_error:.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    print(\"\\nüìã Getting logs...\")\n",
    "    \n",
    "    logs = ml_client.online_deployments.get_logs(\n",
    "        name=\"default\",\n",
    "        endpoint_name=endpoint_name,\n",
    "        lines=100\n",
    "    )\n",
    "    print(logs[-2000:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
